\section{Algorithms}

Reinforcement learning (RL) typically involves an agent interacting with an environment through states, actions, and rewards. In \emph{Q-learning}, the agent learns to approximate an optimal action-value function $Q(s,a)$, which represents the expected discounted reward for taking action $a$ in state $s$. By comparison, \emph{bandit} algorithms (including multi-armed and contextual bandits) consider no or minimal state transitions, learning directly which action maximizes the expected payoff.

\paragraph{Asynchronous Q-learning.}
An \emph{asynchronous} Q-learning agent updates its $Q$-values only for the action actually taken at each step. Let $s_t$ be the current state, $a_t$ the chosen action, and $r_t$ the immediate reward upon transitioning to $s_{t+1}$. The Q-update is:
\begin{equation}
Q(s_t,a_t) 
\;\leftarrow\; 
Q(s_t,a_t) 
\;+\; 
\alpha \Bigl[
  r_t 
  \;+\; 
  \gamma \,\max_{a'} Q(s_{t+1},a') 
  \;-\; 
  Q(s_t,a_t)
\Bigr],
\label{eq:qlearning_async}
\end{equation}
where $\alpha$ is the learning rate and $\gamma$ the discount factor. Only $Q(s_t,a_t)$ changes, reflecting the experience from action $a_t$.

\paragraph{Synchronous Q-learning.}
In \emph{synchronous} Q-learning, the agent updates the Q-values for \emph{all} actions from the same state. Let $A$ be the set of possible actions. If the agent took action $a_t$ but also computes hypothetical rewards $r_t(a)$ for each $a \in A$, the synchronous update is:
\begin{equation}
Q\bigl(s_t,a\bigr) 
\;\leftarrow\; 
\bigl(1-\alpha\bigr)\,Q\bigl(s_t,a\bigr) 
\;+\; 
\alpha \Bigl[
  r_t(a) 
  \;+\; 
  \gamma \,\max_{a'} Q\bigl(s_{t+1},a'\bigr)
\Bigr]
\quad 
\forall a \in A.
\label{eq:qlearning_sync}
\end{equation}
Thus every $Q(s_t,a)$ is updated in a single step, including actions the agent did not actually choose.

\paragraph{Boltzmann Exploration.}
When selecting actions, \emph{Boltzmann} (or \emph{softmax}) exploration draws an action $a$ from a distribution favoring higher estimated $Q$-values:
\begin{equation}
P\bigl(a \mid s\bigr) 
\;=\;
\frac{\exp\!\bigl(\beta\,Q(s,a)\bigr)}{\sum_{b}\,\exp\!\bigl(\beta\,Q(s,b)\bigr)},
\label{eq:boltzmann}
\end{equation}
where $\beta > 0$ is a temperature parameter. Larger $\beta$ makes the distribution flatter, promoting more exploration; smaller $\beta$ makes the policy greedier with respect to $Q$.

\paragraph{Epsilon-greedy Exploration.}
An alternative is \emph{$\varepsilon$-greedy} exploration, which selects the action that maximizes $Q(s,a)$ with probability $1-\varepsilon$ and picks an action uniformly at random with probability $\varepsilon$. Formally,
\begin{equation}
P\bigl(a \mid s\bigr)
\;=\;
\begin{cases}
1-\varepsilon, & \text{if }a = \displaystyle\arg\max_{b}\,Q(s,b),\\
\displaystyle\frac{\varepsilon}{|A|}, & \text{otherwise}.
\end{cases}
\label{eq:egreedy}
\end{equation}
As $\varepsilon$ diminishes, the policy exploits more aggressively based on current $Q$-estimates. In our experiments, $\varepsilon$ (or the Boltzmann temperature $\beta$) starts near 1.0 to encourage exploration and decays linearly over the first 90\% of the episodes to a near-zero value in the final phase, thus allowing the agents to exploit what they have learned.

\paragraph{Bandits.}
A \emph{multi-armed bandit} scenario omits state transitions, focusing instead on learning which of several actions (arms) maximizes expected reward. Let $\hat{\mu}_a$ be the estimated mean reward of arm $a$, and $u_a$ be an uncertainty term. A typical approach is Upper Confidence Bound (UCB), which selects
\begin{equation}
a_t 
\;=\; 
\arg\max_{a}\Bigl[\hat{\mu}_a + u_a\Bigr],
\end{equation}
then updates $\hat{\mu}_a$ and $u_a$ using the reward observed after pulling arm $a$. This encourages exploration of actions whose rewards are still uncertain. In our implementation, we initialize $\mathbf{A}_a = \lambda \mathbf{I}$ and $\mathbf{b}_a = \mathbf{0}$ for each arm $a$. The regularization parameter $\lambda$ ensures that $\mathbf{A}_a$ is invertible from the start and provides numerical stability.

\paragraph{Contextual Bandits.}
\emph{Contextual bandits} generalize the bandit problem by providing a context vector $\mathbf{x}\in\mathbb{R}^d$ prior to choosing an action. The \emph{LinUCB} algorithm assumes a linear payoff model, so each action $a$ has parameters $\boldsymbol{\theta}_a$, and the reward is approximately $\boldsymbol{\theta}_a^\top \mathbf{x}$. For each action $a$, one maintains a matrix $\mathbf{A}_a$ and vector $\mathbf{b}_a$. The algorithm selects $a$ via:
\begin{equation}
a_t 
\;=\; 
\arg\max_{a}\Bigl[\hat{\boldsymbol{\theta}}_{a}^\top \mathbf{x} 
\;+\;
c\sqrt{\mathbf{x}^\top \mathbf{A}_a^{-1}\mathbf{x}}\Bigr],
\quad
\text{where}
\quad
\hat{\boldsymbol{\theta}}_{a} \;=\; \mathbf{A}_a^{-1}\mathbf{b}_a,
\end{equation}
and $c>0$ controls exploration. After observing the reward, $\mathbf{A}_a$ and $\mathbf{b}_a$ are updated, refining the estimate of $\boldsymbol{\theta}_a$ and thus improving action selection over time.

A few other implementation choices may affect performance. Each experiment runs for a set number of \emph{episodes}, during which agents collect rewards and update their parameters. Both $\varepsilon$-greedy and Boltzmann exploration typically begin with higher randomness (e.g., $\varepsilon=1.0$ or high temperature $\beta$) and decay linearly or exponentially across the first 90\% of episodes, settling to zero thereafter for purely greedy actions in the final phase. Q-tables may be initialized to all zeros or to small random values, influencing early exploration. In any given state, if multiple actions share the same $Q$-value, ties are broken uniformly at random. In the bandit setting, particularly with \emph{LinUCB}, two key parameters shape the agent’s behavior: the regularization parameter $\lambda$, which scales the identity matrix added to the design matrix $\mathbf{X}^\top \mathbf{X}$ to stabilize estimates, and the exploration coefficient $c$, which controls the width of the confidence interval around each action’s estimated reward. Larger $c$ promotes more exploration, whereas smaller $c$ favors exploiting the current reward estimates. 
