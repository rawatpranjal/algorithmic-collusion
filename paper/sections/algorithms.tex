\section{Algorithms}

Reinforcement learning (RL) typically involves an agent interacting with an environment through states, actions, and rewards. In \emph{Q-learning}, the agent learns to approximate an optimal action-value function $Q(s,a)$, which represents the expected discounted reward for taking action $a$ in state $s$. By comparison, \emph{bandit} algorithms (including multi-armed and contextual bandits) consider no or minimal state transitions, learning directly which action maximizes the expected payoff.

\paragraph{Asynchronous Q-learning.}
An \emph{asynchronous} Q-learning agent updates its $Q$-values only for the action actually taken at each step. Let $s_t$ be the current state, $a_t$ the chosen action, and $r_t$ the immediate reward upon transitioning to $s_{t+1}$. The Q-update is:
\begin{equation}
Q(s_t,a_t) 
\;\leftarrow\; 
Q(s_t,a_t) 
\;+\; 
\alpha \Bigl[
  r_t 
  \;+\; 
  \gamma \,\max_{a'} Q(s_{t+1},a') 
  \;-\; 
  Q(s_t,a_t)
\Bigr],
\label{eq:qlearning_async}
\end{equation}
where $\alpha$ is the learning rate and $\gamma$ the discount factor. Only $Q(s_t,a_t)$ changes, reflecting the experience from action $a_t$.

\paragraph{Synchronous Q-learning.}
In \emph{synchronous} Q-learning, the agent updates the Q-values for \emph{all} actions from the same state. Let $A$ be the set of possible actions. If the agent took action $a_t$ but also computes hypothetical rewards $r_t(a)$ for each $a \in A$, the synchronous update is:
\begin{equation}
Q\bigl(s_t,a\bigr) 
\;\leftarrow\; 
\bigl(1-\alpha\bigr)\,Q\bigl(s_t,a\bigr) 
\;+\; 
\alpha \Bigl[
  r_t(a) 
  \;+\; 
  \gamma \,\max_{a'} Q\bigl(s_{t+1},a'\bigr)
\Bigr]
\quad 
\forall a \in A.
\label{eq:qlearning_sync}
\end{equation}
Thus every $Q(s_t,a)$ is updated in a single step, including actions the agent did not actually choose.

\paragraph{Boltzmann Exploration.}
When selecting actions, \emph{Boltzmann} (or \emph{softmax}) exploration draws an action $a$ from a distribution favoring higher estimated $Q$-values:
\begin{equation}
P\bigl(a \mid s\bigr) 
\;=\;
\frac{\exp\!\bigl(\beta\,Q(s,a)\bigr)}{\sum_{b}\,\exp\!\bigl(\beta\,Q(s,b)\bigr)},
\label{eq:boltzmann}
\end{equation}
where $\beta > 0$ is a temperature parameter. Larger $\beta$ makes the distribution flatter, promoting more exploration; smaller $\beta$ makes the policy greedier with respect to $Q$.

\paragraph{Epsilon-greedy Exploration.}
An alternative is \emph{$\varepsilon$-greedy} exploration, which selects the action that maximizes $Q(s,a)$ with probability $1-\varepsilon$ and picks an action uniformly at random with probability $\varepsilon$. Formally,
\begin{equation}
P\bigl(a \mid s\bigr)
\;=\;
\begin{cases}
1-\varepsilon, & \text{if }a = \displaystyle\arg\max_{b}\,Q(s,b),\\
\displaystyle\frac{\varepsilon}{|A|}, & \text{otherwise}.
\end{cases}
\label{eq:egreedy}
\end{equation}
As $\varepsilon$ diminishes, the policy exploits more aggressively based on current $Q$-estimates. In our experiments, $\varepsilon$ (or the Boltzmann temperature $\beta$) starts near 1.0 to encourage exploration and decays linearly over the first 90\% of the episodes to a near-zero value in the final phase, thus allowing the agents to exploit what they have learned.

\paragraph{Bandits.}
A \emph{multi-armed bandit} scenario omits state transitions, focusing instead on learning which of several actions (arms) maximizes expected reward. Let $\hat{\mu}_a$ be the estimated mean reward of arm $a$, and $u_a$ be an uncertainty term. A typical approach is Upper Confidence Bound (UCB), which selects
\begin{equation}
a_t 
\;=\; 
\arg\max_{a}\Bigl[\hat{\mu}_a + u_a\Bigr],
\end{equation}
then updates $\hat{\mu}_a$ and $u_a$ using the reward observed after pulling arm $a$. This encourages exploration of actions whose rewards are still uncertain. In our implementation, we initialize $\mathbf{A}_a = \lambda \mathbf{I}$ and $\mathbf{b}_a = \mathbf{0}$ for each arm $a$. The regularization parameter $\lambda$ ensures that $\mathbf{A}_a$ is invertible from the start and provides numerical stability.

\paragraph{Contextual Bandits.}
\emph{Contextual bandits} generalize the bandit problem by providing a context vector $\mathbf{x}\in\mathbb{R}^d$ prior to choosing an action. The \emph{LinUCB} algorithm assumes a linear payoff model, so each action $a$ has parameters $\boldsymbol{\theta}_a$, and the reward is approximately $\boldsymbol{\theta}_a^\top \mathbf{x}$. For each action $a$, one maintains a matrix $\mathbf{A}_a$ and vector $\mathbf{b}_a$. The algorithm selects $a$ via:
\begin{equation}
a_t 
\;=\; 
\arg\max_{a}\Bigl[\hat{\boldsymbol{\theta}}_{a}^\top \mathbf{x} 
\;+\;
c\sqrt{\mathbf{x}^\top \mathbf{A}_a^{-1}\mathbf{x}}\Bigr],
\quad
\text{where}
\quad
\hat{\boldsymbol{\theta}}_{a} \;=\; \mathbf{A}_a^{-1}\mathbf{b}_a,
\end{equation}
and $c>0$ controls exploration. After observing the reward, $\mathbf{A}_a$ and $\mathbf{b}_a$ are updated, refining the estimate of $\boldsymbol{\theta}_a$ and thus improving action selection over time.

A few other implementation choices may affect performance. Each experiment runs for a set number of \emph{episodes}, during which agents collect rewards and update their parameters. Both $\varepsilon$-greedy and Boltzmann exploration typically begin with higher randomness (e.g., $\varepsilon=1.0$ or high temperature $\beta$) and decay linearly or exponentially across the first 90\% of episodes, settling to zero thereafter for purely greedy actions in the final phase. Q-tables may be initialized to all zeros or to small random values, influencing early exploration. In any given state, if multiple actions share the same $Q$-value, ties are broken uniformly at random. In the bandit setting, particularly with \emph{LinUCB}, two key parameters shape the agent's behavior: the regularization parameter $\lambda$, which scales the identity matrix added to the design matrix $\mathbf{X}^\top \mathbf{X}$ to stabilize estimates, and the exploration coefficient $c$, which controls the width of the confidence interval around each action's estimated reward. Larger $c$ promotes more exploration, whereas smaller $c$ favors exploiting the current reward estimates.

\subsection{Budget-Constrained Pacing (Experiment~4)}

Experiment~4 replaces the unconstrained bidding of prior experiments with \emph{budget-constrained pacing agents}. Each agent~$i$ has a total budget $B^i$ over a horizon of $T$ rounds. At each round~$t$, the agent observes its private valuation $v_t^i$ and applies a \emph{pacing multiplier} $\lambda_t^i \in [0.01,\, 1.5]$ to shade its bid. A hard budget cap is enforced so spending never exceeds the remaining budget:
\begin{equation}
b_t^i \;=\; \min\!\bigl(\lambda_t^i\, v_t^i,\; B^i - {\textstyle\sum_{\tau<t}} c_\tau^i\bigr),
\label{eq:hard_cap}
\end{equation}
where $c_\tau^i$ is the payment made by agent~$i$ in round~$\tau$ (zero unless $i$ wins).

\paragraph{Multiplicative Pacing (Lagrangian dual ascent).}
Following Balseiro and Gur (2019), the pacing multiplier is derived from a Lagrangian dual variable $\mu_t^i \geq 0$:
\begin{align}
\mu_{t+1}^i &\;=\; \max\!\bigl(0,\; \mu_t^i + \alpha_p\,(c_t^i - B^i/T)\bigr), \label{eq:dual_update}\\
\lambda_t^i &\;=\; \bigl(1 + \mu_t^i\bigr)^{-1}, \label{eq:mult_lambda}
\end{align}
where $\alpha_p = \text{aggressiveness}/\!\sqrt{T}$ is the dual step size. When the per-round cost exceeds the average budget $B^i/T$, the dual variable rises, reducing $\lambda$ and hence future bids; underspending reverses this adjustment.

\paragraph{PID Pacing (Proportional-Integral controller).}
An alternative controller computes the signed spending error and applies proportional--integral (PI) corrections:
\begin{align}
e_t^i &\;=\; \tfrac{t}{T}\,B^i \;-\; {\textstyle\sum_{\tau \le t}} c_\tau^i, \label{eq:pid_error}\\
\lambda_{t+1}^i &\;=\; \operatorname{clip}\!\bigl(\lambda_t^i + K_P\,e_t^i + K_I\!\textstyle\sum_{\tau} e_\tau^i,\; 0.01,\; 1.5\bigr), \label{eq:pid_update}
\end{align}
where $K_P = 0.30\times\text{aggressiveness}$ and $K_I = 0.05\times\text{aggressiveness}$. A positive error (underspending) raises $\lambda$, encouraging higher bids; overspending drives $\lambda$ down. The derivative term is omitted ($K_D = 0$) following the literature consensus that it is counterproductive in stochastic auction environments.

\paragraph{Feedback loop structure.}
Both algorithms implement the same closed-loop pacing structure illustrated in Figure~\ref{fig:pacing_loop}: the multiplier governs bids, the auction determines payments, and the control law updates the multiplier in response. They differ only in how the error signal is defined and propagated.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  node distance=1.8cm and 2.5cm,
  block/.style={rectangle, draw, rounded corners, minimum width=2.6cm,
                minimum height=0.9cm, text centered, font=\small},
  arrow/.style={-{Stealth[length=2mm]}, thick},
]
  % Nodes
  \node[block] (mult)   {Multiplier $\lambda_t$};
  \node[block, right=of mult] (bid)    {Bid $b_t = \min(\lambda_t v_t,\,\text{rem})$};
  \node[block, right=of bid]  (auction){Auction};
  \node[block, below=1.2cm of auction] (cost)  {Cost $c_t$};
  \node[block, below=1.2cm of mult]  (ctrl)  {Control law};

  % Forward path
  \draw[arrow] (mult)    -- (bid);
  \draw[arrow] (bid)     -- (auction);
  \draw[arrow] (auction) -- (cost);
  \draw[arrow] (cost)    -- (ctrl);
  \draw[arrow] (ctrl)    -- node[left,font=\scriptsize]{$\lambda_{t+1}$} (mult);

  % Labels on control arrow
  \node[font=\scriptsize, align=center, right=0.15cm of ctrl]
    {PID: PI error\\Mult: dual ascent};
\end{tikzpicture}
\caption{Pacing feedback loop shared by both algorithms. The multiplier scales bids; auction outcomes feed back through the control law to update $\lambda$.}
\label{fig:pacing_loop}
\end{figure}

Budget tightness is parameterised as $\text{budget\_tightness} \times \mathbb{E}[v]\times T$, where $\mathbb{E}[v]=0.5$ for uniform marginals, matching the normalisation in Balseiro and Gur (2019). Note that $\eta$ throughout this paper denotes the affiliation parameter (Experiments~2--4); the dual step size above uses $\alpha_p$ to avoid notation collision.
