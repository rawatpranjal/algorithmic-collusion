Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
DoubleML Workflow
DoubleML Workflow
The following steps, which we call the DoubleML workflow, are intended to provide a rough structure for causal analyses with DoubleML. After a short explanation of the idea of each step, we illustrate their meaning in the 401(k) example. In case you are interested in more details of the 401(k) example, you can visit the Python and R Notebooks that are available online.

0. Problem Formulation
The initial step of the DoubleML workflow is to formulate the causal problem at hand. Before we start our statistical analysis, we have to explicitly state the conditions required for a causal interpretation of the estimator, which will be computed later. In many cases, directed acyclical graphs (DAGs) are helpful to formulate the causal problem, illustrate the involved causal channels and the critical parts of the inferential framework. At this stage, a precise argumentation and discussion of the research question is crucial.

DAG for the 401(k) Example
In the 401(k) study, we are interested in estimating the average treatment effect of participation in so-called 401(k) pension plans on employees’ net financial assets. Because we cannot rely on a properly conducted randomized control study in this example, we have to base our analysis on observational data. Hence, we have to use an identification strategy that is based on appropriately controlling for potential confounders. A complication that arises in the 401(k) example is due to so-called endogeneity of the treatment assignment. The treatment variable is an employee’s participation in a 401(k) pension plan which is a decision made by employees and likely to be affected by unobservable effects. For example, is seems reasonable that persons with higher income have stronger preferences to save and also to participate in a pension plan. If our analysis does not account for this self-selection into treatment, the estimated effect is likely to be biased.

To circumvent the endogenous treatment problem, it is possible to exploit randomness in eligibility for 401(k) plans. In other words, the access to the treatment can be considered as randomly assigned once we control for confounding variables. Earlier studies in this context argue that if characteristics that are related to saving preferences are taken into account, eligibility can be considered as good as randomly assigned (at the time 401(k) plans were introduced). The conditional exogeneity in the access to treatment makes it possible to estimate the causal effect of interest by using an instrumental variable (IV) approach. However, for the sake of brevity, we will focus on the so-called intent-to-treat effect in the following. This effect corresponds to the average treatment effect of eligibility (= the instrument) on net financial assets (= the outcome) and is of great interest in many applications. The IV analysis is available in the Python and R Notebooks that are available online.

The previous discussion focuses on the causal problem. Let’s also talk about the statistical methods used for estimation. For identification of the average treatment effect of participation or eligibility on assets, it is crucial that we appropriately account for the confounding factors. That’s where the machine learning tools come into play. Of course, we could simply estimate the causal effect by using a classical linear (IV) regression model: The researcher has to manually pick and, perhaps, transform the confounding variables in the regression model. However, machine learning techniques offer greater flexibility in terms of a more data-driven specification of the main regression equation and the first stage.

1. Data Backend
In Step 1., we initialize the data backend and thereby declare the role of the outcome, the treatment, and the confounding variables.

We use data from the 1991 Survey of Income and Program Participation which is available via the function fetch_401K (Python) or fetch_401k (R). The data backend can be initialized from various data frame objects in Python and R. To estimate the intent-to-treat effect in the 401(k) example, we use eligibility (e401) as the treatment variable of interest. The outcome variable is net_tfa and we control for confounding variables ['age', 'inc', 'educ', 'fsize', 'marr', 'twoearn', 'db', 'pira', 'hown'].


Python
from doubleml import DoubleMLData

from doubleml.datasets import fetch_401K

data = fetch_401K(return_type='DataFrame')

# Construct DoubleMLData object
dml_data = DoubleMLData(data, y_col='net_tfa', d_cols='e401',
                        x_cols=['age', 'inc', 'educ', 'fsize', 'marr',
                                'twoearn', 'db', 'pira', 'hown'])


R
2. Causal Model
In Step 2. we choose a causal model. There are several models currently implemented in DoubleML which differ in terms of the underlying causal structure (e.g., including IV variables or not) and the underlying assumptions.

DoubleML Models
According to the previous discussion, we are interested in estimation of the effect of eligibility on net financial assets. Hence, we do not need to use a model with both a treatment and instrumental variable. There are two potential models, the partially linear regression model (PLR) and the interactive regression model (IRM). These models differ in terms of the type of the treatment variable (continuous vs. binary treatment) and the assumptions underlying the regression equation. For example, the PLR assumes a partially linear structure, whereas the IRM allows treatment effects to be heterogeneous across individuals. To keep the presentation short, we will choose a partially linear model.

3. ML Methods
In Step 3., we can specify the machine learning tools used for estimation of the nuisance parts. We can generally choose any learner from scikit learn in Python and from the mlr3 ecosystem in R.

There are two nuisance parts in the PLR, 
 and 
. In this example, let us specify a random forest and an xgboost learner for both prediction problems. We can directly pass the parameters during initialization of the learner objects. Because we have a binary treatment variable, we can use a classification learner for the corresponding nuisance part. We use a regression learner for the continuous outcome variable net financial assets.


Python
# Random forest learners
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

ml_l_rf = RandomForestRegressor(n_estimators = 500, max_depth = 7,
                                max_features = 3, min_samples_leaf = 3)


ml_m_rf = RandomForestClassifier(n_estimators = 500, max_depth = 5,
                                max_features = 4, min_samples_leaf = 7)


# Xgboost learners
from xgboost import XGBClassifier, XGBRegressor

ml_l_xgb = XGBRegressor(objective = "reg:squarederror", eta = 0.1,
                        n_estimators =35)


ml_m_xgb = XGBClassifier(use_label_encoder = False ,
                        objective = "binary:logistic",
                        eval_metric = "logloss",
                        eta = 0.1, n_estimators = 34)


R
4. DML Specifications
In Step 4., we initialize and parametrize the model object which will later be used to perform the estimation.

We initialize a DoubleMLPLR (Python) / DoubleMLPLR (R) object using the previously generated data-backend. Moreover, we specify the resampling (= the number of repetitions and folds for repeated cross-fitting), the dml algorithm (DML1 vs. DML2) and the score function (“partialling out” or “IV-type”).


Python
from doubleml import DoubleMLPLR

np.random.seed(123)

# Default values
dml_plr_tree = DoubleMLPLR(dml_data,
                            ml_l = ml_l_rf,
                            ml_m = ml_m_rf)


np.random.seed(123)

# Parametrized by user
dml_plr_tree = DoubleMLPLR(dml_data,
                            ml_l = ml_l_rf,
                            ml_m = ml_m_rf,
                            n_folds = 3,
                            n_rep = 1,
                            score = 'partialling out')


R
5. Estimation
We perform estimation in Step 5. In this step, the cross-fitting algorithm is executed such that the predictions in the score are computed. As an output, users can access the coefficient estimates and standard errors either via the corresponding fields or via a summary.


Python
# Estimation
dml_plr_tree.fit()
Out[16]: <doubleml.plm.plr.DoubleMLPLR at 0x7fccdbb5ae40>

# Coefficient estimate
dml_plr_tree.coef
Out[17]: array([8909.63407762])

# Standard error
dml_plr_tree.se
Out[18]: array([1321.82228913])

# Summary
dml_plr_tree.summary
Out[19]: 
             coef      std err  ...        2.5 %        97.5 %
e401  8909.634078  1321.822289  ...  6318.909997  11500.358158

[1 rows x 6 columns]

R
6. Inference
In Step 6., we can perform further inference methods and finally interpret our findings. For example, we can set up confidence intervals or, in case multiple causal parameters are estimated, adjust the analysis for multiple testing. DoubleML supports various approaches to perform valid simultaneous inference which are partly based on a multiplier bootstrap.

To conclude the analysis on the intent-to-treat effect in the 401(k) example, i.e., the average treatment effect of eligibility for 401(k) pension plans on net financial assets, we find a positive and significant effect: Being eligible for such a pension plan increases the amount of net financial assets by approximately 
. This estimate is much smaller than the unconditional effect of eligibility on net financial assets: If we did not control for the confounding variables, the average treatment effect would correspond to 
.


Python
# Summary
dml_plr_tree.summary
Out[20]: 
             coef      std err  ...        2.5 %        97.5 %
e401  8909.634078  1321.822289  ...  6318.909997  11500.358158

[1 rows x 6 columns]

# Confidence intervals
dml_plr_tree.confint()
Out[21]: 
            2.5 %        97.5 %
e401  6318.909997  11500.358158

# Multiplier bootstrap (relevant in case with multiple treatment variables)
dml_plr_tree.bootstrap()
Out[22]: <doubleml.plm.plr.DoubleMLPLR at 0x7fccdbb5ae40>

# Simultaneous confidence bands
dml_plr_tree.confint(joint = True)
Out[23]: 
            2.5 %        97.5 %
e401  6075.923943  11743.344212

R
7. Sensitivity Analysis
In Step 7., we can analyze the sensitivity of the estimated parameters. In the Partially linear regression model (PLR) the causal interpretation relies on conditional exogeneity, which requires to control for confounding variables. The DoubleML python package implements Sensitivity analysis with respect to omitted confounders.

Analyzing the sensitivity of the intent-to-treat effect in the 401(k) example, we find that the effect remains positive even after adjusting for omitted confounders with a lower bound of 
 for the point estimate and 
 including statistical uncertainty.


Python
# Sensitivity analysis
dml_plr_tree.sensitivity_analysis(cf_y=0.04, cf_d=0.03)
Out[24]: <doubleml.plm.plr.DoubleMLPLR at 0x7fccdbb5ae40>

# Sensitivity summary
print(dml_plr_tree.sensitivity_summary)
================== Sensitivity Analysis ==================

------------------ Scenario          ------------------
Significance Level: level=0.95
Sensitivity parameters: cf_y=0.04; cf_d=0.03, rho=1.0

------------------ Bounds with CI    ------------------
         CI lower  theta lower        theta   theta upper      CI upper
e401  2359.496777  4610.983759  8909.634078  13208.284397  15430.415812

------------------ Robustness Values ------------------
      H_0    RV (%)   RVa (%)
e401  0.0  7.029209  5.233154
previous

Getting Started

next

User Guide

 On this page
0. Problem Formulation
1. Data Backend
2. Causal Model
3. ML Methods
4. DML Specifications
5. Estimation
6. Inference
7. Sensitivity Analysis
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
1. The basics of double/debiased machine learning
In the following we provide a brief summary of and motivation to the double machine learning (DML) framework and show how the corresponding methods provided by the DoubleML package can be applied. For details we refer to Chernozhukov et al. (2018).

Note

Detailed notebooks containing the complete code for the examples can be found in the Example Gallery.

1.1. Data generating process
We consider the following partially linear model

 
 
 
with covariates 
, where 
 is a matrix with entries 
. We are interested in performing valid inference on the causal parameter 
. The true parameter 
 is set to 
 in our simulation experiment.

The nuisance functions are given by

 
 
 
 
 
 
 
Note

In Python the data can be generated with doubleml.datasets.make_plr_CCDDHNR2018().

In R the data can be generated with DoubleML::make_plr_CCDDHNR2018().


Python
import numpy as np
from doubleml.datasets import make_plr_CCDDHNR2018

np.random.seed(1234)
n_rep = 1000
n_obs = 500
n_vars = 20
alpha = 0.5

data = list()

for i_rep in range(n_rep):
    (x, y, d) = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars, return_type='array')
    data.append((x, y, d))

R
1.2. Regularization bias in simple ML-approaches
Naive inference that is based on a direct application of machine learning methods to estimate the causal parameter, 
, is generally invalid. The use of machine learning methods introduces a bias that arises due to regularization. A simple ML approach is given by randomly splitting the sample into two parts. On the auxiliary sample indexed by 
 the nuisance function 
 is estimated with an ML method, for example a random forest learner. Given the estimate 
, the final estimate of 
 is obtained as (
) using the other half of observations indexed with 

 
 
 
 
The following figure shows the distribution of the resulting estimates 
 for the simple ML approach (the corresponding notebooks are available in the Example Gallery).


Python
Distribution with non-orthogonal score

R
The regularization bias in the simple ML-approach is caused by the slow convergence of 

i.e., slower than 
. The driving factor is the bias that arises by learning 
 with a random forest or any other ML technique. A heuristic illustration is given by

 
 
 
 
 
 
  
 
 
 
 
 
 
 
 is approximately Gaussian under mild conditions. However, 
 (the regularization bias) diverges in general.

1.3. Overcoming regularization bias by orthogonalization
To overcome the regularization bias we can partial out the effect of 
 from 
 to obtain the orthogonalized regressor 
. We then use the final estimate

 
 
 
 
The following figure shows the distribution of the resulting estimates 
 without sample-splitting (the corresponding notebooks are available in the Example Gallery).


Python
Distribution without sample-splitting

R
If the nuisance models 
 and 
 are estimated on the whole dataset, which is also used for obtaining the final estimate 
, another bias is observed.

1.4. Sample splitting to remove bias induced by overfitting
Using sample splitting, i.e., estimate the nuisance models 
 and 
 on one part of the data (training data) and estimate 
 on the other part of the data (test data), overcomes the bias induced by overfitting. We can exploit the benefits of cross-fitting by switching the role of the training and test sample. Cross-fitting performs well empirically because the entire sample can be used for estimation.

The following figure shows the distribution of the resulting estimates 
 with orthogonal score and sample-splitting (the corresponding notebooks are available in the Example Gallery).


Python
Distribution with orthogonal scores and sample-splitting

R
1.5. Double/debiased machine learning
To illustrate the benefits of the auxiliary prediction step in the DML framework we write the error as

Chernozhukov et al. (2018) argues that:

The first term

 
 
will be asymptotically normally distributed.

The second term

 
 
vanishes asymptotically for many data generating processes.

The third term 
 vanishes in probability if sample splitting is applied. Finally, let us compare all distributions.


Python
All distributions

R
The DoubleML implementation implements a several orthogonal scores and directly applies cross-fitting. The complete code is available in the Example Gallery.


Python
theta_dml = np.full(n_rep, np.nan)
se_dml = np.full(n_rep, np.nan)

for i_rep in range(n_rep):
    (x, y, d) = data[i_rep]
    obj_dml_data = DoubleMLData.from_arrays(x, y, d)
    obj_dml_plr = DoubleMLPLR(
        obj_dml_data,
        ml_l=LGBMRegressor(n_estimators=300, learning_rate=0.1),
        ml_m=LGBMRegressor(n_estimators=200, learning_rate=0.1),
        ml_g=LGBMRegressor(n_estimators=300, learning_rate=0.1),
        n_folds=2,
        score='IV-type')
    obj_dml_plr.fit()
    theta_dml[i_rep] = obj_dml_plr.coef[0]
    se_dml[i_rep] = obj_dml_plr.se[0]

R
1.6. Partialling out score
Another debiased estimator, based on the partialling-out approach of Robinson(1988), is

 
 
 
 
with 
. All nuisance parameters for the estimator with score='partialling out' are conditional mean functions, which can be directly estimated using ML methods. This is a minor advantage over the estimator with score='IV-type'. In the following, we repeat the above analysis with score='partialling out'. In a first part of the analysis, we estimate 
 without sample splitting. Again we observe a bias from overfitting.

The following figure shows the distribution of the resulting estimates 
 without sample-splitting (the corresponding notebooks are available in the Example Gallery).


Python
Distribution without sample splitting

R
Using sample splitting, overcomes the bias induced by overfitting.


Python
Distribution with orthogonal score and sample-splitting

R
Again, the implementation automatically applies cross-fitting. The complete code is available in the Example Gallery.


Python
theta_dml_po = np.full(n_rep, np.nan)
se_dml_po = np.full(n_rep, np.nan)

for i_rep in range(n_rep):
    (x, y, d) = data[i_rep]
    obj_dml_data = DoubleMLData.from_arrays(x, y, d)
    obj_dml_plr = DoubleMLPLR(
        obj_dml_data,
        ml_l=LGBMRegressor(n_estimators=300, learning_rate=0.1),
        ml_m=LGBMRegressor(n_estimators=200, learning_rate=0.1),
        n_folds=2,
        score='partialling out')
    obj_dml_plr.fit()
    theta_dml_po[i_rep] = obj_dml_plr.coef[0]
    se_dml_po[i_rep] = obj_dml_plr.se[0]

R
Finally, let us compare all distributions.


Python
All distributions with partialling-out score

R
1.7. References
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68. doi:10.1111/ectj.12097.

Robinson, P. M. (1988). Root-N-consistent semi-parametric regression. Econometrica 56, 931-54. doi:10.2307/1912705.

previous

User Guide

next

2. Data Backend

 On this page
1.1. Data generating process
1.2. Regularization bias in simple ML-approaches
1.3. Overcoming regularization bias by orthogonalization
1.4. Sample splitting to remove bias induced by overfitting
1.5. Double/debiased machine learning
1.6. Partialling out score
1.7. References
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.


Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
2. Data Backend
DoubleML generally provides interfaces to dataframes as well as arrays.

2.1. DoubleMLData
The usage of both interfaces is demonstrated in the following. We download the Bonus data set from the Pennsylvania Reemployment Bonus experiment.

Note

In Python we use pandas.DataFrame and numpy.ndarray. The data can be fetched via doubleml.datasets.fetch_bonus().

In R we use data.table::data.table(), data.frame(), and matrix(). The data can be fetched via DoubleML::fetch_bonus()


Python
from doubleml.datasets import fetch_bonus

# Load data
df_bonus = fetch_bonus('DataFrame')

df_bonus.head(5)
Out[3]: 
   index   abdt  tg  inuidur1  inuidur2  ...  lusd  husd  muld  dep1  dep2
0      0  10824   0  2.890372        18  ...     0     1     0   0.0   1.0
1      3  10824   0  0.000000         1  ...     1     0     0   0.0   0.0
2      4  10747   0  3.295837        27  ...     1     0     0   0.0   0.0
3     11  10607   1  2.197225         9  ...     0     0     1   0.0   0.0
4     12  10831   0  3.295837        27  ...     1     0     0   1.0   0.0

[5 rows x 26 columns]

R
2.1.1. DoubleMLData from dataframes
The DoubleMLData class serves as data-backend and can be initialized from a dataframe by specifying the column y_col='inuidur1' serving as outcome variable 
, the column(s) d_cols = 'tg' serving as treatment variable 
 and the columns x_cols specifying the confounders.

Note

In Python we use pandas.DataFrame and the API reference can be found here doubleml.DoubleMLData.

In R we use data.table::data.table() and the API reference can be found here DoubleML::DoubleMLData.

For initialization from the R base class data.frame() the API reference can be found here DoubleML::double_ml_data_from_data_frame().


Python
from doubleml import DoubleMLData

# Specify the data and the variables for the causal model
obj_dml_data_bonus = DoubleMLData(df_bonus,
                                  y_col='inuidur1',
                                  d_cols='tg',
                                  x_cols=['female', 'black', 'othrace', 'dep1', 'dep2',
                                          'q2', 'q3', 'q4', 'q5', 'q6', 'agelt35', 'agegt54',
                                          'durable', 'lusd', 'husd'],
                                  use_other_treat_as_covariate=True)


print(obj_dml_data_bonus)
================== DoubleMLData Object ==================

------------------ Data summary      ------------------
Outcome variable: inuidur1
Treatment variable(s): ['tg']
Covariates: ['female', 'black', 'othrace', 'dep1', 'dep2', 'q2', 'q3', 'q4', 'q5', 'q6', 'agelt35', 'agegt54', 'durable', 'lusd', 'husd']
Instrument variable(s): None
No. Observations: 5099

------------------ DataFrame info    ------------------
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5099 entries, 0 to 5098
Columns: 26 entries, index to dep2
dtypes: float64(3), int64(23)
memory usage: 1.0 MB

R
Comments on detailed specifications:

If x_cols is not specified, all variables (columns of the dataframe) which are neither specified as outcome variable y_col, nor treatment variables d_cols, nor instrumental variables z_cols are used as covariates.

In case of multiple treatment variables, the boolean use_other_treat_as_covariate indicates whether the other treatment variables should be added as covariates in each treatment-variable-specific learning task.

Instrumental variables for IV models have to be provided as z_cols.

2.1.2. DoubleMLData from arrays and matrices
To introduce the array interface we generate a data set consisting of confounding variables X, an outcome variable y and a treatment variable d

Note

In python we use numpy.ndarray. and the API reference can be found here doubleml.DoubleMLData.from_arrays().

In R we use the R base class matrix() and the API reference can be found here DoubleML::double_ml_data_from_matrix().


Python
import numpy as np

# Generate data
np.random.seed(3141)

n_obs = 500

n_vars = 100

theta = 3

X = np.random.normal(size=(n_obs, n_vars))

d = np.dot(X[:, :3], np.array([5, 5, 5])) + np.random.standard_normal(size=(n_obs,))

y = theta * d + np.dot(X[:, :3], np.array([5, 5, 5])) + np.random.standard_normal(size=(n_obs,))

R
To specify the data and the variables for the causal model from arrays we call


Python
from doubleml import DoubleMLData

obj_dml_data_sim = DoubleMLData.from_arrays(X, y, d)

print(obj_dml_data_sim)
================== DoubleMLData Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100']
Instrument variable(s): None
No. Observations: 500

------------------ DataFrame info    ------------------
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Columns: 102 entries, X1 to d
dtypes: float64(102)
memory usage: 398.6 KB

R
2.2. Special Data Types
The DoubleMLData class is extended by the following classes to support special data types or allow for additional parameters.

2.2.1. DoubleMLPanelData
The DoubleMLPanelData class serves as data-backend for DiD models and can be initialized from a dataframe. The class is a subclass of DoubleMLData and inherits all methods and attributes. Furthermore, it provides additional methods and attributes to handle panel data ()

id_col: column to with unique identifiers for each unit

t_col: column to specify the time periods of the observation

datetime_unit: unit of the time periods (e.g. ‘Y’, ‘M’, ‘D’, ‘h’, ‘m’, ‘s’)

Note

The t_col can contain float, int or datetime values.


Python
from doubleml.did.datasets import make_did_CS2021

np.random.seed(42)

df = make_did_CS2021(n_obs=500)

dml_data = dml.data.DoubleMLPanelData(
    df,
    y_col="y",
    d_cols="d",
    id_col="id",
    t_col="t",
    x_cols=["Z1", "Z2", "Z3", "Z4"],
    datetime_unit="M"
)


print(dml_data)
================== DoubleMLPanelData Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['Z1', 'Z2', 'Z3', 'Z4']
Instrument variable(s): None
Time variable: t
Id variable: id
No. Observations: 500

------------------ DataFrame info    ------------------
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2500 entries, 0 to 2499
Columns: 10 entries, id to Z4
dtypes: datetime64[s](2), float64(7), int64(1)
memory usage: 195.4 KB
previous

1. The basics of double/debiased machine learning

next

3. Models

 On this page
2.1. DoubleMLData
2.1.1. DoubleMLData from dataframes
2.1.2. DoubleMLData from arrays and matrices
2.2. Special Data Types
2.2.1. DoubleMLPanelData
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
3. Models
The DoubleML-package includes the following models.

3.1. Partially linear models (PLM)
The partially linear models (PLM) take the form

where treatment effects are additive with some sort of linear form.

3.1.1. Partially linear regression model (PLR)
Partially linear regression (PLR) models take the form

 
 
 
where 
 is the outcome variable and 
 is the policy variable of interest. The high-dimensional vector 
 consists of other confounding covariates, and 
 and 
 are stochastic errors.

digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor="#56B4E9"]
       D [fillcolor="#F0E442"]
       V [fillcolor="#F0E442"]
       X [fillcolor="#D55E00"]
     }
     Y -> D -> V [dir="back"];
     X -> D;
     Y -> X [dir="back"];
}
Causal diagram
DoubleMLPLR implements PLR models. Estimation is conducted via its fit() method.

Note

Remark that the standard approach with score='partialling out' does not rely on a direct estimate of 
, but 
.


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

np.random.seed(1111)

data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

print(dml_plr_obj.fit())
================== DoubleMLPLR Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: partialling out

------------------ Machine learner   ------------------
Learner ml_l: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_m: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_l RMSE: [[1.18356413]]
Learner ml_m RMSE: [[1.06008533]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef  std err         t         P>|t|     2.5 %    97.5 %
d  0.512672  0.04491  11.41566  3.492417e-30  0.424651  0.600694

R
3.1.2. Partially linear IV regression model (PLIV)
Partially linear IV regression (PLIV) models take the form

 
 
 
where 
 is the outcome variable, 
 is the policy variable of interest and 
 denotes one or multiple instrumental variables. The high-dimensional vector 
 consists of other confounding covariates, and 
 and 
 are stochastic errors.

digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor="#56B4E9"]
       D [fillcolor="#56B4E9"]
       Z [fillcolor="#F0E442"]
       V [fillcolor="#F0E442"]
       X [fillcolor="#D55E00"]
     }

     Z -> V [dir="back"];
     D -> X [dir="back"];
     Y -> D [dir="both"];
     X -> Y;
     Z -> X [dir="back"];
     Z -> D;

     { rank=same; Y D }
     { rank=same; Z X }
         { rank=same; V }
}
Causal diagram
DoubleMLPLIV implements PLIV models. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_pliv_CHS2015

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

ml_r = clone(learner)

np.random.seed(2222)

data = make_pliv_CHS2015(alpha=0.5, n_obs=500, dim_x=20, dim_z=1, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd', z_cols='Z1')

dml_pliv_obj = dml.DoubleMLPLIV(obj_dml_data, ml_l, ml_m, ml_r)

print(dml_pliv_obj.fit())
================== DoubleMLPLIV Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): ['Z1']
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: partialling out

------------------ Machine learner   ------------------
Learner ml_l: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_m: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_r: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_l RMSE: [[1.48390784]]
Learner ml_m RMSE: [[0.53209683]]
Learner ml_r RMSE: [[1.25240463]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err         t         P>|t|     2.5 %    97.5 %
d  0.481705  0.084337  5.711638  1.118938e-08  0.316407  0.647004

R
3.2. Interactive regression models (IRM)
The interactive regression model (IRM) take the form

where treatment effects are fully heterogeneous.

3.2.1. Binary Interactive Regression Model (IRM)
Interactive regression (IRM) models take the form

 
 
 
where the treatment variable is binary, 
. We consider estimation of the average treatment effects when treatment effects are fully heterogeneous.

Target parameters of interest in this model are the average treatment effect (ATE),

and the average treatment effect of the treated (ATTE),

digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor="#56B4E9"]
       D [fillcolor="#F0E442"]
       V [fillcolor="#F0E442"]
       X [fillcolor="#D55E00"]
     }
     Y -> D -> V [dir="back"];
     X -> D;
     Y -> X [dir="back"];
}
Causal diagram
DoubleMLIRM implements IRM models. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

print(dml_irm_obj.fit())
================== DoubleMLIRM Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: ATE

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(max_depth=5, max_features=10, min_samples_leaf=2)
Learner ml_m: RandomForestClassifier(max_depth=5, max_features=10, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_g0 RMSE: [[1.07085301]]
Learner ml_g1 RMSE: [[1.09682314]]
Classification:
Learner ml_m Log Loss: [[0.55863386]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.599297   0.1887  3.175931  0.001494  0.229452  0.969141

R
3.2.2. Average Potential Outcomes (APOs)
For general discrete-values treatments 
 the model can be generalized to

 
 
 
where 
 is an indicator variable for treatment level 
 and 
 denotes the corresponding propensity score.

Possible target parameters of interest in this model are the average potential outcomes (APOs)

DoubleMLAPO implements the estimation of average potential outcomes. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_apo_obj = dml.DoubleMLAPO(obj_dml_data, ml_g, ml_m, treatment_level=0)

print(dml_apo_obj.fit())
================== DoubleMLAPO Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: APO

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(max_depth=5, max_features=10, min_samples_leaf=2)
Learner ml_m: RandomForestClassifier(max_depth=5, max_features=10, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_g_d_lvl0 RMSE: [[1.09351167]]
Learner ml_g_d_lvl1 RMSE: [[1.07085301]]
Classification:
Learner ml_m Log Loss: [[0.55863386]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
      coef   std err         t     P>|t|     2.5 %    97.5 %
d -0.13084  0.137165 -0.953884  0.340142 -0.399679  0.137999
3.2.3. Average Potential Outcomes (APOs) for Multiple Treatment Levels
If multiple treatment levels should be estimated simulatenously, another possible target parameter of interest in this model are contrasts (or average treatment effects) between treatment levels 
 and 
:

DoubleMLAPOS implements the estimation of average potential outcomes for multiple treatment levels. Estimation is conducted via its fit() method. The causal_contrast() method allows to estimate causal contrasts between treatment levels:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_apos_obj = dml.DoubleMLAPOS(obj_dml_data, ml_g, ml_m, treatment_levels=[0, 1])

print(dml_apos_obj.fit())
================== DoubleMLAPOS Object ==================

------------------ Fit summary       ------------------
       coef   std err         t     P>|t|     2.5 %    97.5 %
0 -0.152706  0.186689 -0.817967  0.413376 -0.518610  0.213199
1  0.533283  0.134784  3.956588  0.000076  0.269112  0.797454

causal_contrast_model = dml_apos_obj.causal_contrast(reference_levels=0)

print(causal_contrast_model.summary)
            coef   std err         t     P>|t|     2.5 %   97.5 %
1 vs 0  0.685989  0.231734  2.960236  0.003074  0.231798  1.14018
3.2.4. Interactive IV model (IIVM)
Interactive IV regression (IIVM) models take the form

 
 
 
where the treatment variable is binary, 
 and the instrument is binary, 
. Consider the functions 
, 
 and 
, where 
 maps the support of 
 to 
 and 
 and 
 respectively map the support of 
 and 
 to 
 for some 
, such that

 
 
 
The target parameter of interest in this model is the local average treatment effect (LATE),

 
digraph {
     nodesep=1;
     ranksep=1;
     rankdir=LR;
     { node [shape=circle, style=filled]
       Y [fillcolor="#56B4E9"]
       D [fillcolor="#56B4E9"]
       Z [fillcolor="#F0E442"]
       V [fillcolor="#F0E442"]
       X [fillcolor="#D55E00"]
     }

     Z -> V [dir="back"];
     D -> X [dir="back"];
     Y -> D [dir="both"];
     X -> Y;
     Z -> X [dir="back"];
     Z -> D;

     { rank=same; Y D }
     { rank=same; Z X }
         { rank=same; V }
}
Causal diagram
DoubleMLIIVM implements IIVM models. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_iivm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_r = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

np.random.seed(4444)

data = make_iivm_data(theta=0.5, n_obs=1000, dim_x=20, alpha_x=1.0, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd', z_cols='z')

dml_iivm_obj = dml.DoubleMLIIVM(obj_dml_data, ml_g, ml_m, ml_r)

print(dml_iivm_obj.fit())
================== DoubleMLIIVM Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): ['z']
No. Observations: 1000

------------------ Score & algorithm ------------------
Score function: LATE

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_m: RandomForestClassifier(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_r: RandomForestClassifier(max_depth=5, max_features=20, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_g0 RMSE: [[1.12983057]]
Learner ml_g1 RMSE: [[1.13102231]]
Classification:
Learner ml_m Log Loss: [[0.69684828]]
Learner ml_r0 Log Loss: [[0.69508862]]
Learner ml_r1 Log Loss: [[0.43503345]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.389126  0.23113  1.683581  0.092263 -0.063881  0.842132

--------------- Additional Information ----------------
Robust Confidence Set: [-0.0935, 0.8324]

R
3.3. Difference-in-Differences Models (DID)
Difference-in-Differences Models (DID) implemented in the package focus on the the binary treatment case with staggered adoption.

Note

The notation and identifying assumptions are based on Callaway and Sant’Anna (2021), but adjusted to better fit into the general package documentation conventions, sometimes slightly abusing notation. The underlying score functions are based on Sant’Anna and Zhao (2020), Zimmert (2018) and Chang (2020). For a more detailed introduction and recent developments of the difference-in-differences literature see e.g. Roth et al. (2022).

We consider 
 observed units at time periods 
. The treatment status for unit 
 at time period 
 is denoted by the binary variable 
. The package considers the staggered adoption setting, where a unit stays treated after it has been treated once (Irreversibility of Treatment).

Let 
 be an indicator variable that takes value one if unit 
 is treated at time period 
, 
 with 
 refering to the first post-treatment period. I units are never exposed to the treatment, define 
.

The target parameters are defined in terms of differences in potential outcomes. The observed and potential outcome for each unit 
 at time period 
 are assumed to be of the form

 
such that we observe one consistent potential outcome for each unit at each time period.

The corresponding target parameters are the average causal effects of the treatment

This target parameter quantifies the average change in potential outcomes for units that are treated the first time in period 
 with the difference in outcome being evaluated for time period 
. The corresponding control groups, defined by an indicator 
, can be typically set as either the never treated or not yet treated units. Let

 
 
 
The corresponding identifying assumptions are:

Irreversibility of Treatment:
 For all 
, 
 implies 

Panel Data (Random Sampling):
 is independent and identically distributed.

Limited Treatment Anticipation:
There is a known 
 such that 
 for all 
 such that 
.

Conditional Parallel Trends:
Let 
 be defined as in Assumption 3.\ For each 
 and 
 such that 
:

Never Treated:

Not Yet Treated:

Overlap:
For each time period 
 and 
 there exists a 
 such that 
 and 

Note

For a detailed discussion of the assumptions see Callaway and Sant’Anna (2021).

Under the assumptions above (either Assumption 4.a or 4.b), the target parameter 
 is identified see Theorem 1. Callaway and Sant’Anna (2021).

3.3.1. Panel data
For the estimation of the target parameters 
 the following nuisance functions are required:

 
 
 
where 
 denotes the population outcome regression function and 
 the generalized propensity score. The interpretation of the parameters is as follows:

 is the first post-treatment period of interest, i.e. the treatment group.

 is the pre-treatment period, i.e. the time period from which the conditional parallel trends are assumed.

 is the time period of interest or evaluation period, i.e. the time period where the treatment effect is evaluated.

 is number of anticipation periods, i.e. the number of time periods for which units are assumed to anticipate the treatment.

Note

Remark that the nuisance functions depend on the control group used for the estimation of the target parameter. By slight abuse of notation we use the same notation for both control groups 
 and 
. More specifically, the control group only depends on 
 for not yet treated units.

Under these assumptions the target parameter 
 can be estimated by choosing a suitable combination of 
 if 
, i.e. the parallel trends are assumed to hold at least one period more than the anticipation period.

Note

The choice 
 corresponds to the definition of 
 from Callaway and Sant’Anna (2021).

As an example, if the target parameter is the effect on the group receiving treatment in 
 but evaluated in 
 with an anticipation period of 
, then the pre-treatment period is 
. The parallel trend assumption is slightly stronger with anticipation as the trends have to parallel for a longer periods, i.e. 
.

In the following, we will omit the subscript 
 in the notation of the nuisance functions and the control group (implicitly assuming 
).

For a given tuple 
 the target parameter 
 is estimated by solving the empirical version of the the following linear moment condition:

 
with nuisance elements 
 and score function 
 being defined in section Panel Data. Under the identifying assumptions above

DoubleMLDIDMulti implements the estimation of 
 for multiple time periods and requires DoubleMLPanelData as input. Setting gt_combinations='standard' will estimate the target parameter for all (possible) combinations of 
 with 
 and 
 with 
 and 
. This corresponds to the setting where all trends are set as short as possible, but still respecting the anticipation period.

Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.did.datasets import make_did_CS2021

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

np.random.seed(42)

df = make_did_CS2021(n_obs=500)

dml_data = dml.data.DoubleMLPanelData(
    df,
    y_col="y",
    d_cols="d",
    id_col="id",
    t_col="t",
    x_cols=["Z1", "Z2", "Z3", "Z4"],
    datetime_unit="M"
)


dml_did_obj = dml.did.DoubleMLDIDMulti(
    obj_dml_data=dml_data,
    ml_g=RandomForestRegressor(min_samples_split=10),
    ml_m=RandomForestClassifier(min_samples_split=10),
    gt_combinations="standard",
    control_group="never_treated",
)


print(dml_did_obj.fit())
================== DoubleMLDIDMulti Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['Z1', 'Z2', 'Z3', 'Z4']
Instrument variable(s): None
Time variable: t
Id variable: id
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: observational
Control group: never_treated
Anticipation periods: 0

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(min_samples_split=10)
Learner ml_m: RandomForestClassifier(min_samples_split=10)
Out-of-sample Performance:
Regression:
Learner ml_g0 RMSE: [[ 4.18516129  3.93074943  7.02260304 11.27461519  4.07788588  4.29168951
   3.95311164  8.07229774  3.79330022  3.90145324  4.00518448  3.89472978]]
Learner ml_g1 RMSE: [[3.11942111 3.42094064 6.87309461 9.51494845 4.31910229 3.59563003
  3.86897905 7.36696349 3.97619643 3.94427158 3.77746575 4.38470495]]
Classification:
Learner ml_m Log Loss: [[0.70344386 0.72269685 0.70305686 0.70557077 0.7204309  0.69572427
  0.70774361 0.72987186 0.76535102 0.78386025 0.74475816 0.79953099]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
                                  coef   std err  ...     2.5 %    97.5 %
ATT(2025-03,2025-01,2025-02) -0.698642  0.489488  ... -1.658021  0.260738
ATT(2025-03,2025-02,2025-03)  0.211002  0.496591  ... -0.762299  1.184303
ATT(2025-03,2025-02,2025-04)  0.900127  0.938263  ... -0.938836  2.739089
ATT(2025-03,2025-02,2025-05)  2.001603  1.568932  ... -1.073447  5.076653
ATT(2025-04,2025-01,2025-02)  0.046507  0.430465  ... -0.797189  0.890204
ATT(2025-04,2025-02,2025-03)  0.108870  0.479959  ... -0.831833  1.049573
ATT(2025-04,2025-03,2025-04)  0.952146  0.463816  ...  0.043082  1.861210
ATT(2025-04,2025-03,2025-05)  2.918293  0.891527  ...  1.170933  4.665653
ATT(2025-05,2025-01,2025-02) -0.082905  0.468449  ... -1.001049  0.835239
ATT(2025-05,2025-02,2025-03) -0.035689  0.643679  ... -1.297276  1.225899
ATT(2025-05,2025-03,2025-04)  0.270248  0.443672  ... -0.599334  1.139830
ATT(2025-05,2025-04,2025-05)  1.186775  0.542268  ...  0.123950  2.249601

[12 rows x 6 columns]
Note

Remark that the output contains two different outcome regressions 
 and 
. As in the IRM model the outcome regression 
 refers to the control group, whereas 
 refers to the outcome regression for the treatment group, i.e.

 
 
 
Further, 
 is only required for Sensitivity Analysis and is not used for the estimation of the target parameter.

Note

A more detailed example is available in the Example Gallery.

3.3.2. Repeated cross-sections
Note

Will be implemented soon.

3.3.3. Effect Aggregation
The following section considers the aggregation of different 
 to summary measures based on Callaway and Sant’Anna (2021). All implemented aggregation schemes take the form of a weighted average of the 
 estimates

 
 
where 
 is a weight function based on the treatment group 
 and time period 
. The aggragation schemes are implmented via the aggregate() method of the DoubleMLDIDMulti class.


Python
import numpy as np

import doubleml as dml

from doubleml.did.datasets import make_did_CS2021

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

np.random.seed(42)

df = make_did_CS2021(n_obs=500)

dml_data = dml.data.DoubleMLPanelData(
    df,
    y_col="y",
    d_cols="d",
    id_col="id",
    t_col="t",
    x_cols=["Z1", "Z2", "Z3", "Z4"],
    datetime_unit="M"
)


dml_did_obj = dml.did.DoubleMLDIDMulti(
    obj_dml_data=dml_data,
    ml_g=RandomForestRegressor(min_samples_split=10),
    ml_m=RandomForestClassifier(min_samples_split=10),
    gt_combinations="standard",
    control_group="never_treated",
)


dml_did_obj.fit()
Out[9]: <doubleml.did.did_multi.DoubleMLDIDMulti at 0x7fccdbb05550>

agg_did_obj = dml_did_obj.aggregate(aggregation="group")

agg_did_obj.aggregated_frameworks.bootstrap()
Out[11]: <doubleml.double_ml_framework.DoubleMLFramework at 0x7fccdba0a210>

print(agg_did_obj)
================== DoubleMLDIDAggregation Object ==================
 Group Aggregation 

------------------ Overall Aggregated Effects ------------------
    coef  std err        t    P>|t|   2.5 %   97.5 %
1.389489 0.537438 2.585394 0.009727 0.33613 2.442847
------------------ Aggregated Effects         ------------------
             coef   std err         t     P>|t|     2.5 %    97.5 %
2025-03  1.037577  0.943693  1.099485  0.271556 -0.812028  2.887182
2025-04  1.935220  0.642648  3.011323  0.002601  0.675653  3.194786
2025-05  1.186775  0.542268  2.188541  0.028630  0.123950  2.249601
------------------ Additional Information     ------------------
Score function: observational
Control group: never_treated
Anticipation periods: 0
The method aggregate() requires the aggregation argument to be set to one of the following values:

'group': aggregates 
 estimates by the treatment group 
.

'time': aggregates 
 estimates by the time period 
 (based on group size).

'eventstudy': aggregates 
 estimates based on time difference to first treatment assignment like an event study (based on group size).

dictionary: a dictionary with values containing the aggregation weights (as numpy.ma.MaskedArray).

Note

A more detailed example on effect aggregation is available in the example gallery. For a detailed discussion on different aggregation schemes, we refer to of Callaway and Sant’Anna (2021).

3.3.4. Two treatment periods
Warning

This documentation refers to the deprecated implementation for two time periods. This functionality will be removed in a future version.

Note

We recommend using the implementation Panel data and Repeated cross-sections.

Difference-in-Differences Models (DID) implemented in the package focus on the the binary treatment case with with two treatment periods.

Adopting the notation from Sant’Anna and Zhao (2020), let 
 be the outcome of interest for unit 
 at time 
. Further, let 
 indicate if unit 
 is treated before time 
 (otherwise 
). Since all units start as untreated (
), define 
 Relying on the potential outcome notation, denote 
 as the outcome of unit 
 at time 
 if the unit did not receive treatment up until time 
 and analogously for 
 with treatment. Consequently, the observed outcome for unit is 
 at time 
 is 
. Further, let 
 be a vector of pre-treatment covariates.

Target parameter of interest is the average treatment effect on the treated (ATTE)

The corresponding identifying assumptions are

(Cond.) Parallel Trends: 

Overlap: 
: 
 and 

Note

For a more detailed introduction and recent developments of the difference-in-differences literature see e.g. Roth et al. (2022).

3.3.4.1. Panel Data
If panel data are available, the observations are assumed to be iid. of form 
. Remark that the difference 
 has to be defined as the outcome y in the DoubleMLData object.

DoubleMLIDID implements difference-in-differences models for panel data. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.did.datasets import make_did_SZ2020

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)

ml_m = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=5)

np.random.seed(42)

data = make_did_SZ2020(n_obs=500, return_type='DataFrame')

# y is already defined as the difference of observed outcomes
obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_did_obj = dml.DoubleMLDID(obj_dml_data, ml_g, ml_m)

print(dml_did_obj.fit())
================== DoubleMLDID Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['Z1', 'Z2', 'Z3', 'Z4']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: observational

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(max_depth=5, min_samples_leaf=5)
Learner ml_m: RandomForestClassifier(max_depth=5, min_samples_leaf=5)
Out-of-sample Performance:
Regression:
Learner ml_g0 RMSE: [[16.27429763]]
Learner ml_g1 RMSE: [[13.35731523]]
Classification:
Learner ml_m Log Loss: [[0.66601815]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err         t     P>|t|     2.5 %    97.5 %
d -2.840718  1.760386 -1.613691  0.106595 -6.291011  0.609575
3.3.4.2. Repeated cross-sections
For repeated cross-sections, the observations are assumed to be iid. of form 
, where 
 is a dummy variable if unit 
 is observed pre- or post-treatment period, such that the observed outcome can be defined as

Further, treatment and covariates are assumed to be stationary, such that the joint distribution of 
 is invariant to 
.

DoubleMLIDIDCS implements difference-in-differences models for repeated cross-sections. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.did.datasets import make_did_SZ2020

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)

ml_m = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=5)

np.random.seed(42)

data = make_did_SZ2020(n_obs=500, cross_sectional_data=True, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd', t_col='t')

dml_did_obj = dml.DoubleMLDIDCS(obj_dml_data, ml_g, ml_m)

print(dml_did_obj.fit())
================== DoubleMLDIDCS Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['Z1', 'Z2', 'Z3', 'Z4']
Instrument variable(s): None
Time variable: t
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: observational

------------------ Machine learner   ------------------
Learner ml_g: RandomForestRegressor(max_depth=5, min_samples_leaf=5)
Learner ml_m: RandomForestClassifier(max_depth=5, min_samples_leaf=5)
Out-of-sample Performance:
Regression:
Learner ml_g_d0_t0 RMSE: [[17.4915707]]
Learner ml_g_d0_t1 RMSE: [[44.85397773]]
Learner ml_g_d1_t0 RMSE: [[32.74938952]]
Learner ml_g_d1_t1 RMSE: [[53.7282094]]
Classification:
Learner ml_m Log Loss: [[0.67936506]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
     coef   std err         t     P>|t|      2.5 %    97.5 %
d -4.9944  7.561785 -0.660479  0.508947 -19.815226  9.826426
3.4. Sample Selection Models (SSM)
Sample Selection Models (SSM) implemented in the package focus on the the binary treatment case when outcomes are only observed for a subpopulation due to sample selection or outcome attrition.

The implementation and notation is based on Bia, Huber and Lafférs (2023). Let 
 be the binary treatment indicator and 
 the potential outcome under treatment value 
. Further, define 
 to be the realized outcome and 
 as a binary selection indicator. The outcome 
 is only observed if 
. Finally, let 
 be a vector of observed covariates, measures prior to treatment assignment.

Target parameter of interest is the average treatment effect (ATE)

The corresponding identifying assumption is

Cond. Independence of Treatment: 
 for 

where further assmputions are made in the context of the respective sample selection model.

Note

A more detailed example can be found in the Example Gallery.

3.4.1. Missingness at Random
Consider the following two additional assumptions for the sample selection model:

Cond. Independence of Selection: 
 for 

Common Support: 
 and 
 for 

such that outcomes are missing at random (for the score see Scores).

DoubleMLSSM implements sample selection models. The score score='missing-at-random' refers to the correponding score relying on the assumptions above. The DoubleMLData object has to be defined with the additional argument s_col for the selection indicator. Estimation is conducted via its fit() method:


Python
import numpy as np

from sklearn.linear_model import LassoCV, LogisticRegressionCV

from doubleml.datasets import make_ssm_data

import doubleml as dml

np.random.seed(42)

n_obs = 2000

df = make_ssm_data(n_obs=n_obs, mar=True, return_type='DataFrame')

dml_data = dml.DoubleMLData(df, 'y', 'd', s_col='s')

ml_g = LassoCV()

ml_m = LogisticRegressionCV(penalty='l1', solver='liblinear')

ml_pi = LogisticRegressionCV(penalty='l1', solver='liblinear')

dml_ssm = dml.DoubleMLSSM(dml_data, ml_g, ml_m, ml_pi, score='missing-at-random')

dml_ssm.fit()
Out[13]: <doubleml.irm.ssm.DoubleMLSSM at 0x7fcce97fc140>

print(dml_ssm)
================== DoubleMLSSM Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100']
Instrument variable(s): None
Score/Selection variable: s
No. Observations: 2000

------------------ Score & algorithm ------------------
Score function: missing-at-random

------------------ Machine learner   ------------------
Learner ml_g: LassoCV()
Learner ml_pi: LogisticRegressionCV(penalty='l1', solver='liblinear')
Learner ml_m: LogisticRegressionCV(penalty='l1', solver='liblinear')
Out-of-sample Performance:
Regression:
Learner ml_g_d0 RMSE: [[1.10039862]]
Learner ml_g_d1 RMSE: [[1.11071087]]
Classification:
Learner ml_pi Log Loss: [[0.53791422]]
Learner ml_m Log Loss: [[0.63593298]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.965531  0.065969  14.636048  1.654070e-48  0.836234  1.094829

R
3.4.2. Nonignorable Nonresponse
When sample selection or outcome attriction is realated to unobservables, identification generally requires an instrument for the selection indicator 
. Consider the following additional assumptions for the instrumental variable:

Cond. Correlation: 

Cond. Independence: 
 and 
 for 

This requires the instrumental variable 
, which must not affect 
 or be associated with unobservables affecting 
 conditional on 
 and 
. Further, the selection is determined via a (unknown) threshold model:

Threshold: 
 where 
 is a general function and 
 is a scalar with strictly monotonic cumulative distribution function conditional on 
.

Cond. Independence: 
.

Let 
 denote the selection probability. Additionally, the following assumptions are required:

Common Support for Treatment: 

Cond. Effect Homogeneity: 

Common Support for Selection: 
 for 

For further details, see Bia, Huber and Lafférs (2023).

DAG
Causal paths under nonignorable nonresponse
DoubleMLSSM implements sample selection models. The score score='nonignorable' refers to the correponding score relying on the assumptions above. The DoubleMLData object has to be defined with the additional argument s_col for the selection indicator and z_cols for the instrument. Estimation is conducted via its fit() method:


Python
import numpy as np

from sklearn.linear_model import LassoCV, LogisticRegressionCV

from doubleml.datasets import make_ssm_data

import doubleml as dml

np.random.seed(42)

n_obs = 2000

df = make_ssm_data(n_obs=n_obs, mar=False, return_type='DataFrame')

dml_data = dml.DoubleMLData(df, 'y', 'd', z_cols='z', s_col='s')

ml_g = LassoCV()

ml_m = LogisticRegressionCV(penalty='l1', solver='liblinear')

ml_pi = LogisticRegressionCV(penalty='l1', solver='liblinear')

dml_ssm = dml.DoubleMLSSM(dml_data, ml_g, ml_m, ml_pi, score='nonignorable')

dml_ssm.fit()
Out[27]: <doubleml.irm.ssm.DoubleMLSSM at 0x7fcce97fc620>

print(dml_ssm)
================== DoubleMLSSM Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100']
Instrument variable(s): ['z']
Score/Selection variable: s
No. Observations: 2000

------------------ Score & algorithm ------------------
Score function: nonignorable

------------------ Machine learner   ------------------
Learner ml_g: LassoCV()
Learner ml_pi: LogisticRegressionCV(penalty='l1', solver='liblinear')
Learner ml_m: LogisticRegressionCV(penalty='l1', solver='liblinear')
Out-of-sample Performance:
Regression:
Learner ml_g_d0 RMSE: [[0.92827999]]
Learner ml_g_d1 RMSE: [[1.10079785]]
Classification:
Learner ml_pi Log Loss: [[0.44124313]]
Learner ml_m Log Loss: [[0.59854797]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
      coef   std err         t         P>|t|     2.5 %    97.5 %
d  1.14268  0.183373  6.231467  4.620874e-10  0.783276  1.502084

R
3.5. Regression Discontinuity Designs (RDD)
Regression Discontinuity Designs (RDD) are causal inference methods used when treatment assignment is determined by a continuous running variable (“score”) crossing a known threshold (“cutoff”). These designs exploit discontinuities in the probability of receiving treatment at the cutoff to estimate the average treatment effect. RDDs are divided into two main types: Sharp and Fuzzy.

The key idea behind RDD is that units just above and just below the threshold are assumed to be comparable, differing only in the treatment assignment. This allows estimating the causal effect at the threshold by comparing outcomes of treated and untreated units.

Our implementation follows work from Noack, Olma and Rothe (2024).

Let 
 be the observed outcome of an individual and 
 the treatment it received. By using a set of additional covariates 
 for each observation, 
 and 
 can be adjusted in a first stage, to reduce the standard deviation in the estimation of the causal effect.

Note

To fit into the package syntax, our notation differs as follows from the one used in most standard RDD works (as for example Cattaneo and Titiunik (2022)):
 the score (instead of 
)

 the covariates (instead of 
)

 the treatment received (in sharp RDD instead of 
)

 the treatment assigned (only relevant in fuzzy RDD)

Note

The doubleml.rdd module depends on rdrobust which can be installed via pip install rdrobust or pip install doubleml[rdd].

3.5.1. Sharp Regression Discontinuity Design
In a Sharp RDD, the treatment 
 is deterministically assigned at the cutoff (
𝟙
).

Let 
 represent the score, and let 
 denote the cutoff point. Further, let 
 and 
 denote the potential outcomes with and without treatment, respectively. Then, the treatment effect at the cutoff

is identified as the difference in the conditional expectation of 
 at the cutoff from both sides

  
 
The key assumption for identifying this effect in a sharp RDD is:

Continuity: The conditional mean of the potential outcomes 
 for 
 is continuous at the cutoff level 
.

This includes the necessary condition of exogeneity, implying units cannot perfectly manipulate their value of 
 to either receive or avoid treatment exactly at the cutoff.

Without the use of covariates, 
 is typically estimated by running separate local linear regressions on each side of the cutoff, yielding an estimator of the form:

 
where the 
 are local linear regression weights that depend on the data through the realizations of the running variable only and 
 is a bandwidth.

Under standard conditions, which include that the running variable is continuously distributed, and that the bandwidth 
 tends to zero at an appropriate rate, the estimator 
 is approximately normally distributed in large samples, with bias of order 
 and variance of order 
:

If covariates are available, they can be used to improve the accuracy of empirical RD estimates. The most popular strategy is to include them linearly and without kernel localization in the local linear regression. By simple least squares algebra, this “linear adjustment” estimator can be written as a no-covariates estimator with the covariate-adjusted outcome 
:

 
Here, 
 is the minimizer from the regression

 
 
with 
 (see fs_specification in Implementation Details), 
 with 
 a kernel function.

If 
 is twice continuously differentiable around the cutoff, then the distribution of 
 is similar to the one of the base estimator with potentially smaller variance term 
.

As this linear adjustment might not exploit the available covariate information efficiently, DoubleML features an RDD estimator with flexible covariate adjustment based on potentially nonlinear adjustment functions 
. The estimator takes the following form:

 
Similar to other algorithms in DoubleML, 
 is estimated by ML methods and with crossfitting. Different than in other models, there is no orthogonal score, but a similar global insensitivity property holds (for details see Noack, Olma and Rothe (2024)). We adjust the outcome variable by the influence of the covariates.

This reduces the variance in the estimation potentially even further to:

 
with 
 being a kernel constant. To maximize the precision of the estimator 
 for any particular bandwidth 
, 
 has to be chosen such that 
 is as small as possible. The equally-weighted average of the left and right limits of the conditional expectation function 
 at the cutoff achieves this goal. According to Noack, Olma and Rothe (2024), it holds:

where:

 
RDFlex implements this regression discontinuity design with 
 being estimated by user-specified ML methods. The indicator fuzzy=False indicates a sharp design. The DoubleMLData object has to be defined with the arguments:

y_col refers to the observed outcome, on which we want to estimate the effect at the cutoff

s_col refers to the score

x_cols refers to the covariates to be adjusted for

d_cols is an indicator of whether an observation is treated or not. In the sharp design, this should be identical to an indicator of whether an observation is left or right of the cutoff (
)

Estimation is conducted via its fit() method:


Python
import numpy as np

import pandas as pd

from sklearn.linear_model import LassoCV

from doubleml.rdd.datasets import make_simple_rdd_data

from doubleml.rdd import RDFlex

import doubleml as dml

np.random.seed(42)

data_dict = make_simple_rdd_data(n_obs=1000, fuzzy=False)

cov_names = ['x' + str(i) for i in range(data_dict['X'].shape[1])]

df = pd.DataFrame(np.column_stack((data_dict['Y'], data_dict['D'], data_dict['score'], data_dict['X'])), columns=['y', 'd', 'score'] + cov_names)

dml_data = dml.DoubleMLData(df, y_col='y', d_cols='d', x_cols=cov_names, s_col='score')

ml_g = LassoCV()

rdflex_obj = RDFlex(dml_data, ml_g, fuzzy=False)

rdflex_obj.fit()
Out[14]: <doubleml.rdd.rdd.RDFlex at 0x7fcce978a8d0>

print(rdflex_obj)
Method             Coef.     S.E.     t-stat       P>|t|           95% CI
-------------------------------------------------------------------------
Conventional      1.290     0.565     2.285    2.232e-02  [0.183, 2.396]
Robust                 -        -     2.053    4.005e-02  [0.062, 2.660]
Design Type:        Sharp
Cutoff:             0
First Stage Kernel: triangular
Final Bandwidth:    [0.63117637]
3.5.2. Fuzzy Regression Discontinuity Design
In a Fuzzy RDD, treatment assignment 
 is identical to the sharp RDD (
𝟙
), however, compliance is limited around the cutoff which leads to a different treatment received 
 than assigned (
) for some units.

The parameter of interest in the Fuzzy RDD is the average treatment effect at the cutoff, for all individuals that comply with the assignment

with 
 being the potential outcome under the potential treatments. This effect is identified by

 
The assumptions for identifying the ATT in a fuzzy RDD are:

Continuity of Potential Outcomes: Similar to sharp RDD, the conditional mean of the potential outcomes 
 for 
 is continuous at the cutoff level 
.

Continuity of Treatment Assignment Probability: The probability of receiving treatment 
 must change discontinuously at the cutoff, but there should be no other jumps in the probability.

Monotonicity: There must be no “defiers”, meaning individuals for whom the treatment assignment goes in the opposite direction of the score.

Under similar considerations as in the sharp case, an estimator using flexible covariate adjustment can be derived as:

 
 
where 
 and 
 are defined as in the sharp RDD setting, with the respective outcome.

RDFlex implements this fuzzy RDD with flexible covariate adjustment. The indicator fuzzy=True indicates a fuzzy design. The DoubleMLData object has to be defined with the arguments:

y_col refers to the observed outcome, on which we want to estimate the effect at the cutoff

s_col refers to the score

x_cols refers to the covariates to be adjusted for

d_cols is an indicator of whether an observation is treated or not. In the fuzzy design, this should not be identical to an indicator of whether an observation is left or right of the cutoff (
)

Estimation is conducted via its fit() method:


Python
import numpy as np

import pandas as pd

from sklearn.linear_model import LassoCV, LogisticRegressionCV

from doubleml.rdd.datasets import make_simple_rdd_data

from doubleml.rdd import RDFlex

import doubleml as dml

np.random.seed(42)

data_dict = make_simple_rdd_data(n_obs=1000, fuzzy=True)

cov_names = ['x' + str(i) for i in range(data_dict['X'].shape[1])]

df = pd.DataFrame(np.column_stack((data_dict['Y'], data_dict['D'], data_dict['score'], data_dict['X'])), columns=['y', 'd', 'score'] + cov_names)

dml_data = dml.DoubleMLData(df, y_col='y', d_cols='d', x_cols=cov_names, s_col='score')

ml_g = LassoCV()

ml_m = LogisticRegressionCV()

rdflex_obj = RDFlex(dml_data, ml_g, ml_m, fuzzy=True)

rdflex_obj.fit()
Out[30]: <doubleml.rdd.rdd.RDFlex at 0x7fcce96e2240>

print(rdflex_obj)
Method             Coef.     S.E.     t-stat       P>|t|           95% CI
-------------------------------------------------------------------------
Conventional      3.207     4.935     0.650    5.157e-01  [-6.464, 12.879]
Robust                 -        -     0.682    4.955e-01  [-7.313, 15.111]
Design Type:        Fuzzy
Cutoff:             0
First Stage Kernel: triangular
Final Bandwidth:    [0.61404894]
3.5.3. Implementation Details
There are some specialities in the RDFlex implementation that differ from the rest of the package and thus deserve to be pointed out here.

Bandwidth Selection: The bandwidth is a crucial tuning parameter for RDD algorithms. By default, our implementation uses the rdbwselect method from the rdrobust library for an initial selection. This can be overridden by the user using the parameter h_fs. Since covariate adjustment and RDD fitting are interacting, by default, we repeat the bandwidth selection and nuisance estimation steps once in the fit() method. This can be adjusted by n_iterations.

Kernel Selection: Another crucial decision when estimating with RDD is the kernel determining the weights for observations around the cutoff. For this, the parameters fs_kernel and kernel are important. The latter is a key-worded argument and is used in the RDD estimation, while the fs_kernel specifies the kernel used in the nuisance estimation. By default, both of them are triangular.

Local and Global Learners: RDFlex estimates the nuisance functions locally around the cutoff. In certain scenarios, it can be desirable to rather perform a global fit on the full support of the score 
. For this, the Global Learners in doubleml.utils can be used (see our example notebook in the Example Gallery).

First Stage Specifications: In nuisance estimation, we have to add variable(s) to add information about the location of the observation left or right of the cutoff. Available options are: In the default case fs_specification="cutoff", this is an indicator of whether the observation is left or right. If fs_specification="cutoff and score", additionally the score is added. In the case of fs_specification="interacted cutoff and score", also an interaction term of the cutoff indicator and the score is added.

Intention-to-Treat Effects: Above, we demonstrated how to estimate the ATE at the cutoff in a fuzzy RDD. To estimate an Intention-to-Treat effect instead, the parameter fuzzy=False can be selected.

Key-worded Arguments: rdrobust as the underlying RDD library has additional parameters to tune the estimation. You can use **kwargs to add them via RDFlex.

previous

2. Data Backend

next

4. Heterogeneous treatment effects

 On this page
3.1. Partially linear models (PLM)
3.1.1. Partially linear regression model (PLR)
3.1.2. Partially linear IV regression model (PLIV)
3.2. Interactive regression models (IRM)
3.2.1. Binary Interactive Regression Model (IRM)
3.2.2. Average Potential Outcomes (APOs)
3.2.3. Average Potential Outcomes (APOs) for Multiple Treatment Levels
3.2.4. Interactive IV model (IIVM)
3.3. Difference-in-Differences Models (DID)
3.3.1. Panel data
3.3.2. Repeated cross-sections
3.3.3. Effect Aggregation
3.3.4. Two treatment periods
3.4. Sample Selection Models (SSM)
3.4.1. Missingness at Random
3.4.2. Nonignorable Nonresponse
3.5. Regression Discontinuity Designs (RDD)
3.5.1. Sharp Regression Discontinuity Design
3.5.2. Fuzzy Regression Discontinuity Design
3.5.3. Implementation Details
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.


Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
4. Heterogeneous treatment effects
Most implemented solutions focus on the IRM or IIVM models, as for the PLR and PLIV models heterogeneous treatment effects can be usually modelled via feature construction.

4.1. Group average treatment effects (GATEs)
The DoubleMLIRM and DoubleMLPLR classes contain the gate() method, which enables the estimation and construction of confidence intervals for GATEs after fitting the DoubleML object. To estimate GATEs, the user has to specify a pandas DataFrame containing the groups (dummy coded or one column with strings). This will construct and fit a DoubleMLBLP object. Confidence intervals can then be constructed via the confint() method. Jointly valid confidence intervals will be based on a gaussian multiplier bootstrap.

4.1.1. GATEs for IRM models
Group Average Treatment Effects (GATEs) for DoubleMLIRM models consider the target parameters

where 
 denotes a group indicator and 
 the potential outcome with 
.

Point estimates and confidence intervals can be obtained via the gate() and confint() methods. Remark that for straightforward interpretation, the groups have to be mutually exclusive.


Python
import numpy as np

import pandas as pd

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=5, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

_ = dml_irm_obj.fit()

# define groups
np.random.seed(42)

groups = pd.DataFrame(np.random.choice(3, 500), columns=['Group'], dtype=str)

print(groups.head())
  Group
0     2
1     0
2     2
3     2
4     0

gate_obj = dml_irm_obj.gate(groups=groups)

ci = gate_obj.confint()

print(ci)
            2.5 %    effect    97.5 %
Group_0  0.344440  0.792972  1.241503
Group_1  0.047288  0.647864  1.248441
Group_2  0.132248  0.545930  0.959613
A more detailed notebook on GATEs with DoubleMLIRM models is available in the example gallery.

4.1.2. GATEs for PLR models
Group Average Treatment Effects (GATEs) for DoubleMLPLR models consider a slightly adjusted version of the DoubleMLPLR model. Instead of considering a constant treatment effect 
 for all observations, the adjusted model allows for a different effect based on groups.

 
 
 
where 
 for 
 denotes a group indicator where the groups can depend on the counfounding features 
.

Point estimates and confidence intervals can be obtained via the gate() and confint() methods. Remark that for straightforward interpretation, the groups have to be mutually exclusive.


Python
import numpy as np

import pandas as pd

import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

ml_g = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

dml_data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20)

dml_plr_obj = dml.DoubleMLPLR(dml_data, ml_g, ml_m)

_ = dml_plr_obj.fit()

# define groups
np.random.seed(42)

groups = pd.DataFrame(np.random.choice(3, 500), columns=['Group'], dtype=str)

print(groups.head())
  Group
0     2
1     0
2     2
3     2
4     0

gate_obj = dml_plr_obj.gate(groups=groups)

ci = gate_obj.confint()

print(ci)
            2.5 %    effect    97.5 %
Group_0  0.497964  0.644113  0.790261
Group_1  0.247617  0.395268  0.542919
Group_2  0.452114  0.593981  0.735848
A more detailed notebook on GATEs with DoubleMLPLR models is available in the example gallery.

4.2. Conditional average treatment effects (CATEs)
The DoubleMLIRM and DoubleMLPLR classes contain the cate() method, which enables the estimation and construction of confidence intervals for CATEs after fitting the DoubleML object. To estimate CATEs, the user has to specify a pandas DataFrame containing the basis (e.g. B-splines) for the conditional treatment effects. This will construct and fit a DoubleMLBLP object. Confidence intervals can then be constructed via the confint() method. Jointly valid confidence intervals will be based on a gaussian multiplier bootstrap.

4.2.1. CATEs for IRM models
Conditional Average Treatment Effects (CATEs) for DoubleMLIRM models consider the target parameters

for a low-dimensional feature 
, where 
 the potential outcome with 
.

Point estimates and confidence intervals can be obtained via the gate() and confint() methods.


Python
import numpy as np

import pandas as pd

import patsy

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

_ = dml_irm_obj.fit()

# define a basis with respect to the first variable
design_matrix = patsy.dmatrix("bs(x, df=5, degree=2)", {"x":obj_dml_data.data["X1"]})

spline_basis = pd.DataFrame(design_matrix)

print(spline_basis.head())
     0         1         2         3         4    5
0  1.0  0.068700  0.819223  0.112078  0.000000  0.0
1  1.0  0.000000  0.377147  0.621359  0.001494  0.0
2  1.0  0.179101  0.007421  0.000000  0.000000  0.0
3  1.0  0.000000  0.018092  0.849427  0.132481  0.0
4  1.0  0.562390  0.216943  0.000000  0.000000  0.0

cate_obj = dml_irm_obj.cate(basis=spline_basis)

ci = cate_obj.confint(basis=spline_basis)

print(ci.head())
      2.5 %    effect    97.5 %
0 -0.602322  0.100208  0.802738
1  0.287123  0.754692  1.222261
2 -3.856758 -1.772444  0.311869
3  0.829764  1.343639  1.857515
4 -2.373451 -1.171696  0.030059
A more detailed notebook on CATEs for DoubleMLIRM models is available in the example gallery. The examples also include the construction of a two-dimensional basis with B-splines.

4.2.2. CATEs for PLR models
Conditional Average Treatment Effects (CATEs) for DoubleMLPLR models consider a slightly adjusted version of the DoubleMLPLR model. Instead of considering a constant treatment effect 
 for all observations, the adjusted model allows for a different effect based on groups.

 
 
 
where 
 denotes the heterogeneous treatment effect.

Point estimates and confidence intervals can be obtained via the gate() and confint() methods.


Python
import numpy as np

import pandas as pd

import patsy

import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

dml_data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20)

dml_plr_obj = dml.DoubleMLPLR(dml_data, ml_g, ml_m)

_ = dml_plr_obj.fit()

# define a basis with respect to the first variable
design_matrix = patsy.dmatrix("bs(x, df=5, degree=2)", {"x":obj_dml_data.data["X1"]})

spline_basis = pd.DataFrame(design_matrix)

print(spline_basis.head())
     0         1         2         3         4    5
0  1.0  0.068700  0.819223  0.112078  0.000000  0.0
1  1.0  0.000000  0.377147  0.621359  0.001494  0.0
2  1.0  0.179101  0.007421  0.000000  0.000000  0.0
3  1.0  0.000000  0.018092  0.849427  0.132481  0.0
4  1.0  0.562390  0.216943  0.000000  0.000000  0.0

cate_obj = dml_plr_obj.cate(basis=spline_basis)

ci = cate_obj.confint(basis=spline_basis)

print(ci.head())
      2.5 %    effect    97.5 %
0  0.355627  0.542648  0.729668
1  0.418969  0.553754  0.688540
2 -0.046525  0.529468  1.105461
3  0.415556  0.569449  0.723342
4  0.256082  0.461412  0.666742
A more detailed notebook on CATEs for DoubleMLPLR models is available in the example gallery.

Theory: In the model above, it holds

 
 
 
 
such that

 
 
  
 
 
 
Remark that for the generated 
-agebras 
 implying

and consequently

Consequently, 
 can be estimated by regressing 
 on 
:

 
The DoubleML implementation approximates the effect 
 by a linear projection on a supplied basis 
:

where 
 are coefficients to be estimated. The coverage of the confidence intervals is meant to include the the approximation 
.

4.3. Weighted Average Treatment Effects
The DoubleMLIRM class allows to specify weights via the weights argument in the initialization of the DoubleMLIRM object. Given some weights, 
 the model identifies the weighted average treatment effect

The interpretation depends on the choice of weights. The simplest examples include

 which corresponds to the average treatment effect (ATE)

 
 which corresponds to the group average treatment effect (GATE) for group 

 which corresponds to the average value of policy 
, where 

where the weights 
 only depend on the features 
.

In these cases the weights can be specified as an array via the weights argument in the initialization of the DoubleMLIRM object.


Python
import numpy as np

import pandas as pd

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

weights = np.ones(500)

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m, weights=weights)

_ = dml_irm_obj.fit()

print(dml_irm_obj.summary)
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.599297   0.1887  3.175931  0.001494  0.229452  0.969141
If the weights do not only depend on the features 
, but e.g. also on the treatment 
 estimation becomes more involved. To identifiy the correct parameter not only the weights 
 but also their conditional expectation

has to be specified. A common example is the average treatment effect on the treated (ATTE) which can be identified by setting

 

 
 

which depends on the propensity score 
. In this case the weights can be specified as a dictionary the weights argument in the initialization of the DoubleMLIRM object.

One other important example would be the sensitivity analysis for group average treatment effects on the treated (GATET). In this case the weights would take the following form

 
 

 
 

To simplify the specification of the weights, the DoubleMLIRM with score='ATTE' accepts binary weights, which should correspond to 
. This automatically relies on the propensity score 
 to construct the weights mentioned above (e.g. for weights equal to one this refers to the average treatment effect on the treated).

A more detailed notebook on weighted average treatment effects for on GATE sensitivity analysis is available in the example gallery.

4.4. Quantiles
The DoubleML package includes (local) quantile estimation for potential outcomes for IRM and IIVM models.

4.4.1. Potential quantiles (PQs)
For a quantile 
 the target parameters 
 of interest are the potential quantiles (PQs),

and local potential quantiles (LPQs),

where 
 denotes the potential outcome with 
.

DoubleMLPQ implements potential quantile estimation. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestClassifier

np.random.seed(3141)

ml_g = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_pq_obj = dml.DoubleMLPQ(obj_dml_data, ml_g, ml_m, treatment=1, quantile=0.5)

dml_pq_obj.fit().summary
Out[97]: 
       coef   std err         t     P>|t|     2.5 %    97.5 %
d  0.553878  0.149858  3.696011  0.000219  0.260161  0.847595
DoubleMLLPQ implements local potential quantile estimation, where the argument treatment indicates the potential outcome. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_iivm_data

from sklearn.ensemble import RandomForestClassifier

np.random.seed(3141)

ml_g = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

data = make_iivm_data(theta=0.5, n_obs=2000, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd', z_cols='z')

dml_lpq_obj = dml.DoubleMLLPQ(obj_dml_data, ml_g, ml_m, treatment=1, quantile=0.5)

dml_lpq_obj.fit().summary
Out[108]: 
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.696224  0.61728  1.127889  0.259367 -0.513624  1.906072
4.4.2. Quantile treatment effects (QTEs)
For a quantile 
 the target parameter 
 of interest are the quantile treatment effect (QTE),

where 
 denotes the corresponding potential quantile.

Analogously, the local quantile treatment effect (LQTE) can be defined as the difference of the corresponding local potential quantiles.

DoubleMLQTE implements quantile treatment effect estimation. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestClassifier

np.random.seed(3141)

ml_g = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_qte_obj = dml.DoubleMLQTE(obj_dml_data, ml_g, ml_m, score='PQ', quantiles=[0.25, 0.5, 0.75])

dml_qte_obj.fit().summary
Out[119]: 
          coef   std err         t     P>|t|     2.5 %    97.5 %
0.25  0.274825  0.347310  0.791297  0.428771 -0.405890  0.955541
0.50  0.449150  0.192539  2.332782  0.019660  0.071782  0.826519
0.75  0.709606  0.193308  3.670867  0.000242  0.330731  1.088482
To estimate local quantile effects the score argument has to be set to 'LPQ'. A detailed notebook on PQs and QTEs is available in the example gallery.

4.5. Conditional value at risk (CVaR)
The DoubleML package includes conditional value at risk estimation for IRM models.

4.5.1. CVaR of potential outcomes
For a quantile 
 the target parameters 
 of interest are the conditional values at risk (CVaRs) of the potential outcomes,

 
where 
 denotes the potential outcome with 
 and 
 the corresponding cdf of 
.

DoubleMLCVAR implements conditional value at risk estimation for potential outcomes, where the argument treatment indicates the potential outcome. Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

np.random.seed(3141)

ml_g = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_cvar_obj = dml.DoubleMLCVAR(obj_dml_data, ml_g, ml_m, treatment=1, quantile=0.5)

dml_cvar_obj.fit().summary
Out[130]: 
       coef   std err         t         P>|t|     2.5 %    97.5 %
d  1.588364  0.096616  16.43989  9.909942e-61  1.398999  1.777728
4.5.2. CVaR treatment effects
For a quantile 
 the target parameter 
 of interest are the treatment effects on the conditional value at risk,

where 
 denotes the corresponding conditional values at risk of the potential outcomes.

DoubleMLQTE implements CVaR treatment effect estimation, if the score argument has been set to 'CVaR' (default is 'PQ'). Estimation is conducted via its fit() method:


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

np.random.seed(3141)

ml_g = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=20, max_depth=10, min_samples_leaf=2)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_cvar_obj = dml.DoubleMLQTE(obj_dml_data, ml_g, ml_m, score='CVaR', quantiles=[0.25, 0.5, 0.75])

dml_cvar_obj.fit().summary
Out[141]: 
          coef   std err         t         P>|t|     2.5 %    97.5 %
0.25  0.474731  0.246624  1.924921  5.423921e-02 -0.008642  0.958105
0.50  0.691911  0.143495  4.821855  1.422293e-06  0.410667  0.973156
0.75  1.001714  0.166375  6.020819  1.735369e-09  0.675625  1.327803
A detailed notebook on CVaR estimation for potential outcomes and treatment effects is available in the example gallery.

4.6. Policy Learning with Trees
Policy Learning considers to find an optimal decision policy. We consider deterministic binary policies, which are defined as mapping

Using the score component 
 of the IRM score, we can find the optimal treatment policy by solving the weighted classification problem

 
 
 
where 
 denotes a policy class, which we define as depth-
 classification trees. Thus, we estimate splits in the features 
 that reflect the heterogeneity of the treatment effect and consequently maximize the sum of the estimated individual treatment effects of all individuals by assigning different treatments.

The DoubleMLIRM class contains the policy_tree() method, which enables the estimation of a policy tree using weighted classification after fitting the DoubleMLIRM object. To estimate a policy tree, the user has to specify a pandas DataFrame containing the covariates on based on which the policy will make treatment decisions. These can be either the original covariates used in the DoubleMLIRM estimation, or a subset, or new covariates. This will construct and fit a DoubleMLPolicyTree object. A plot of the decision rules can be displayed by the plot_tree() method. The predict() method enables the application of the estimated policy on new data. The depth parameter, which defaults to 2, can be used to adjust the maximum depth of the tree.


Python
import numpy as np

import pandas as pd

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

_ = dml_irm_obj.fit()

# define features to learn policy on
np.random.seed(42)

features = data[["X1","X2","X3"]]

print(features.head())
         X1        X2        X3
0 -0.368577 -0.688886  0.793315
1  0.078426 -1.028731  0.755885
2 -2.899021 -1.294123 -0.884821
3  0.502005  0.902920 -0.158726
4 -1.843018  0.170705  0.712846

# fits a tree of depth 2
policy_tree_obj = dml_irm_obj.policy_tree(features=features)

policy_tree_obj.plot_tree();
A more detailed notebook on Policy Trees is available in the example gallery.

previous

3. Models

next

5. Score functions

 On this page
4.1. Group average treatment effects (GATEs)
4.1.1. GATEs for IRM models
4.1.2. GATEs for PLR models
4.2. Conditional average treatment effects (CATEs)
4.2.1. CATEs for IRM models
4.2.2. CATEs for PLR models
4.3. Weighted Average Treatment Effects
4.4. Quantiles
4.4.1. Potential quantiles (PQs)
4.4.2. Quantile treatment effects (QTEs)
4.5. Conditional value at risk (CVaR)
4.5.1. CVaR of potential outcomes
4.5.2. CVaR treatment effects
4.6. Policy Learning with Trees
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.


Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
5. Score functions
We use method-of-moments estimators for the target parameter 
 based upon the empirical analog of the moment condition

where we call 
 the score function, 
, 
 is the parameter of interest and 
 denotes nuisance functions with population value 
. We use score functions 
 that satisfy 
 with 
 being the unique solution and that obey the Neyman orthogonality condition

The score functions of many double machine learning models (PLR, PLIV, IRM, IIVM) are linear in the parameter 
, i.e.,

Hence the estimator can be written as

 
The linearity of the score function in the parameter 
 allows the implementation of key components in a very general way. The methods and algorithms to estimate the causal parameters, to estimate their standard errors, to perform a multiplier bootstrap, to obtain confidence intervals and many more are implemented in the abstract base class DoubleML. The object-oriented architecture therefore allows for easy extension to new model classes for double machine learning. This is doable with very minor effort.

If the linearity of the score function is not satisfied, the computations are more involved. In the Python package DoubleML, the functionality around the score functions is implemented in mixin classes called LinearScoreMixin and NonLinearScoreMixin. The R package currently only comes with an implementation for linear score functions. In case of a non-linear score function, the parameter estimate 
 is obtained via numerical root search of the empirical analog of the moment condition 
.

5.1. Implementation of the score function and the estimate of the causal parameter
As an example we consider a partially linear regression model (PLR) implemented in DoubleMLPLR.


Python
import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

np.random.seed(3141)

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

data = make_plr_CCDDHNR2018(alpha=0.5, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

dml_plr_obj.fit();

print(dml_plr_obj)
================== DoubleMLPLR Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: partialling out

------------------ Machine learner   ------------------
Learner ml_l: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_m: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_l RMSE: [[1.12705095]]
Learner ml_m RMSE: [[1.03917696]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.480691  0.040533  11.859129  1.929729e-32  0.401247  0.560135

R
The fit() method of DoubleMLPLR stores the estimate 
 in its coef attribute.


Python
print(dml_plr_obj.coef)
[0.48069071]

R
The values of the score function components 
 and 
 are stored in the attributes psi_elements['psi_a'] and psi_elements['psi_b'] (Python package DoubleML) and psi_a and psi_b (R package DoubleML). In the attribute psi the values of the score function 
 are stored.


Python
print(dml_plr_obj.psi[:5])
[[[ 0.02052929]]

 [[-0.00409412]]

 [[ 0.00138944]]

 [[-0.11208236]]

 [[-0.29678199]]]

R
5.2. Implemented Neyman orthogonal score functions
5.2.1. Partially linear models (PLM)
The following scores for partially linear models are implemented.

5.2.1.1. Partially linear regression model (PLR)
For the PLR model implemented in DoubleMLPLR one can choose between score='partialling out' and score='IV-type'.

score='partialling out' implements the score function:

 
 
 
with 
, where

 
 
 
The components of the linear score are

 
 
 
score='IV-type' implements the score function:

 
 
 
with 
, where

 
 
 
The components of the linear score are

 
 
 
5.2.1.2. Partially linear IV regression model (PLIV)
For the PLIV model implemented in DoubleMLPLIV one can choose between score='IV-type' and score='partialling out'.

score='partialling out' implements the score function:

 
 
 
with 
 and where the components of the linear score are

 
 
 
score='IV-type' implements the score function:

 
 
 
with 
 and where the components of the linear score are

 
 
 
5.2.2. Interactive regression models (IRM)
The following scores for nonparametric regression models are implemented.

5.2.2.1. Binary Interactive Regression Model (IRM)
For the IRM model implemented in DoubleMLIRM one can choose between score='ATE' and score='ATTE'. Furthermore, weights 
 and

can be specified. The general score function takes the form

 
 
 
 
 
with 
 and where the components of the linear score are

 
 
 
 
 
If no weights are specified, score='ATE' sets the weights

 
 
 
whereas score='ATTE' changes weights to:

 
 
 
 
 
This score is identical to the original presentation in Section 5.1. of Chernozhukov et al. (2018)

 
 
 
 
 
 
For more details on other weight specifications, see Weighted Average Treatment Effects.

5.2.2.2. Average Potential Outcomes (APOs)
For the average potential outcomes (APO) models implemented in DoubleMLAPO and DoubleMLAPOS the score='APO' is implemented. Furthermore, weights 
 and

can be specified. For a given treatment level 
 the general score function takes the form

 
 
 
 
with 
, where the true nuisance elements are

 
 
 
The components of the linear score are

 
 
 
 
If no weights are specified, the weights are set to

 
 
 
5.2.2.3. Interactive IV model (IIVM)
For the IIVM model implemented in DoubleMLIIVM we employ for score='LATE' the score function:

score='LATE' implements the score function:

 
 
 
 
 
 
 
with 
 and where the components of the linear score are

 
 
 
 
 
 
 
5.2.2.4. Potential quantiles (PQs)
For DoubleMLPQ the only valid option is score='PQ'. For treatment=d with 
 and a quantile 
 this implements the nonlinear score function:

 
where 
 with true values

 
 
 
Remark that 
 depends on the target parameter 
, such that the score is estimated with a preliminary estimate 
. For further details, see Kallus et al. (2019).

5.2.2.5. Local potential quantiles (LPQs)
For DoubleMLLPQ the only valid option is score='LPQ'. For treatment=d with 
, instrument 
 and a quantile 
 this implements the nonlinear score function:

 
 
 
 
 
 
where 
 with true values

 
 
 
Further, the compliance probability 
 is estimated with the two additional nuisance components

Remark that 
 depends on the target parameter 
, such that the score is estimated with a preliminary estimate 
. For further details, see Kallus et al. (2019).

5.2.2.6. Conditional value at risk (CVaR)
For DoubleMLCVAR the only valid option is score='CVaR'. For treatment=d with 
 and a quantile 
 this implements the score function:

 
where 
 with true values

 
 
 
and 
 being the potential quantile of 
. As for potential quantiles, the estimate 
 is constructed via a preliminary estimate of 
. For further details, see Kallus et al. (2019).

5.2.3. Difference-in-Differences Models
The following scores for difference-in-differences models are implemented.

5.2.3.1. Panel Data
As in the description of the DiD model, the required nuisance elements are

 
 
 
for a certain choice of 
 and 
 and control group 
.

For notational purposes, we will omit the subscripts 
 in the following and use the notation

 (population outcome regression function of the control group)

 (generalized propensity score)

All scores in the multi-period setting have the form

 
 
i.e. the score is only non-zero for units in the corresponding treatment group 
 and control group 
.

For the difference-in-differences model implemented in DoubleMLDIDMulti one can choose between score='observational' and score='experimental'.

score='observational' implements the score function (dropping the unit index 
):

 
 
 
 
 
 
 
 
where the components of the final linear score 
 are

 
 
 
and the nuisance elements 
.

Note

Remark that 
 if 
.

If in_sample_normalization='False', the score is set to

 
 
 
 
 
with 
. Remark that this will result in the same score, but just uses slightly different normalization.

score='experimental' assumes that the treatment probability is independent of the covariates 
 and does not rely on the propensity score. Instead define the population outcome regression for treated and control group as

 (control group)

 (treated group)

score='experimental' implements the score function:

 
 
 
 
 
 
where the components of the final linear score 
 are

 
 
 
and the nuisance elements 
.

Analogously, if in_sample_normalization='False', the score is set to

 
 
 
 
 
with 
. Remark that this will result in the same score, but just uses slightly different normalization.

5.2.3.2. Repeated Cross-Sectional Data
Note

Will be implemented soon.

5.2.3.3. Two treatment periods
Warning

This documentation refers to the deprecated implementation for two time periods. This functionality will be removed in a future version. The generalized version are Panel Data and Repeated Cross-Sectional Data.

5.2.3.3.1. Panel Data
For the difference-in-differences model implemented in DoubleMLDID one can choose between score='observational' and score='experimental'.

score='observational' implements the score function (dropping the unit index 
):

 
 
 
 
 
 
 
 
where the components of the linear score are

 
 
 
 
 
 
 
 
and the nuisance elements 
 are defined as

 
 
 
If in_sample_normalization='False', the score is set to

 
 
 
 
 
with 
, where 
 is estimated on the cross-fitting folds. Remark that this will result in the same score, but just uses slightly different normalization.

score='experimental' assumes that the treatment probability is independent of the covariates 
 and implements the score function:

 
 
 
 
 
 
where the components of the linear score are

 
 
 
 
 
 
and the nuisance elements 
 are defined as

 
 
 
Analogously, if in_sample_normalization='False', the score is set to

 
 
 
 
 
with 
, where 
 is estimated on the cross-fitting folds. Remark that this will result in the same score, but just uses slightly different normalization.

5.2.3.3.2. Repeated Cross-Sectional Data
For the difference-in-differences model implemented in DoubleMLDIDCS one can choose between score='observational' and score='experimental'.

score='observational' implements the score function (dropping the unit index 
):

 
 
 
 
 
 
 
 
 
 
 
where the components of the linear score are

 
 
 
 
 
 
 
 
 
 
 
and the nuisance elements 
 are defined as

If in_sample_normalization='False', the score is set to

 
 
 
 
 
 
 
 
 
with 
, where 
 and 
 are estimated on the cross-fitting folds. Remark that this will result in the same score, but just uses slightly different normalization.

score='experimental' assumes that the treatment probability is independent of the covariates 
 and implements the score function:

 
 
 
 
 
 
 
where the components of the linear score are

 
 
 
 
 
 
 
and the nuisance elements 
 are defined as

 
 
 
Analogously, if in_sample_normalization='False', the score is set to

 
 
 
 
 
 
 
with 
, where 
 and 
 are estimated on the cross-fitting folds. Remark that this will result in the same score, but just uses slightly different normalization.

5.2.4. Sample Selection Models
The following scores for sample selection models are implemented.

5.2.4.1. Missingness at Random
For DoubleMLSSM the score='missing-at-random' implements the score function:

where

 
 
 
 
 
for 
 and 
 with true values

 
 
 
For further details, see Bia, Huber and Lafférs (2023).

5.2.4.2. Nonignorable Nonresponse
For DoubleMLSSM the score='nonignorable' implements the score function:

where

 
 
 
 
 
for 
 and 
 with true values

 
 
 
The estimate of 
 is constructed via a preliminary estimate of 
 via nested cross-fitting.

For further details, see Bia, Huber and Lafférs (2023).

5.3. Specifying alternative score functions via callables
Via callables user-written score functions can be used. This functionality is at the moment only implemented for specific model classes in Python. For the PLR model implemented in DoubleMLPLR an alternative score function can be set via score. Choose a callable object / function with signature score(y, d, g_hat, m_hat, smpls) which returns the two score components 
 and 
.

For example, the non-orthogonal score function

can be obtained with


Python
import numpy as np

def non_orth_score(y, d, l_hat, m_hat, g_hat, smpls):
    u_hat = y - g_hat
    psi_a = -np.multiply(d, d)
    psi_b = np.multiply(d, u_hat)
    return psi_a, psi_b


R
Use DoubleMLPLR with inf_model=non_orth_score in order to obtain the estimator

 
when applying fit(). Note that this estimate will in general be prone to a regularization bias, see also Overcoming regularization bias by orthogonalization.

previous

4. Heterogeneous treatment effects

next

6. Double machine learning algorithms

 On this page
5.1. Implementation of the score function and the estimate of the causal parameter
5.2. Implemented Neyman orthogonal score functions
5.2.1. Partially linear models (PLM)
5.2.2. Interactive regression models (IRM)
5.2.3. Difference-in-Differences Models
5.2.4. Sample Selection Models
5.3. Specifying alternative score functions via callables
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
6. Double machine learning algorithms
The DoubleML package comes with two different algorithms to obtain DML estimates.

Note

The algorithms are argument dml_procedure is deprecated in the python package. Generally, the second version of the algorithm DML2 is recommended, to obtain more stable estimates.

6.1. Algorithm DML1
The algorithm dml_procedure='dml1' can be summarized as

Inputs: Choose a model (PLR, PLIV, IRM, IIVM), provide data 
, a Neyman-orthogonal score function 
 and specify machine learning method(s) for the nuisance function(s) 
.

Train ML predictors on folds: Take a 
-fold random partition 
 of observation indices 
 such that the size of each fold 
 is 
. For each 
, construct a high-quality machine learning estimator

of 
, where 
 depends only on the subset of data 
.

Estimate causal parameter: For each 
, construct the estimator 
 as the solution to the equation

 
 
The estimate of the causal parameter is obtain via aggregation

 
 
Outputs: The estimate of the causal parameter 
 as well as the values of the evaluated score function are returned.

6.2. Algorithm DML2
The algorithm dml_procedure='dml2' can be summarized as

Inputs: Choose a model (PLR, PLIV, IRM, IIVM), provide data 
, a Neyman-orthogonal score function 
 and specify machine learning method(s) for the nuisance function(s) 
.

Train ML predictors on folds: Take a 
-fold random partition 
 of observation indices 
 such that the size of each fold 
 is 
. For each 
, construct a high-quality machine learning estimator

of 
, where 
 depends only on the subset of data 
.

Estimate causal parameter: Construct the estimator for the causal parameter 
 as the solution to the equation

 
 
 
Outputs: The estimate of the causal parameter 
 as well as the values of the evaluate score function are returned.



6.3. Implementation of the double machine learning algorithms
As an example we consider a partially linear regression model (PLR) implemented in DoubleMLPLR. The default version of the DoubleML class is based on the DML2 algorithm.


Python
import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

np.random.seed(3141)

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

data = make_plr_CCDDHNR2018(alpha=0.5, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

dml_plr_obj.fit();

R
The fit() method of DoubleMLPLR stores the estimate 
 in its coef attribute.


Python
dml_plr_obj.coef
Out[13]: array([0.48069071])

R
Let 
. The values of the score function 
 are stored in the attribute psi.


Python
dml_plr_obj.psi[:5]
Out[14]: 
array([[[ 0.02052929]],

       [[-0.00409412]],

       [[ 0.00138944]],

       [[-0.11208236]],

       [[-0.29678199]]])

R
previous

5. Score functions

next

7. Learners, hyperparameters and hyperparameter tuning

 On this page
6.1. Algorithm DML1
6.2. Algorithm DML2
6.3. Implementation of the double machine learning algorithms
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
7. Learners, hyperparameters and hyperparameter tuning
The estimation of a double/debiased machine learning model involves the estimation of several nuisance function with machine learning estimators. Such learners are implemented in various Python and R packages. The implementation of DoubleML is based on the meta-packages scikit-learn for Python and mlr3 for R. The interfaces to specify the learners, set hyperparameters and tune hyperparameters are described in the following separately for Python and R.

7.1. Python: Learners and hyperparameters
7.1.1. Minimum requirements for learners
The minimum requirement for a learner to be used for nuisance models in the DoubleML package is

The implementation of a fit() and predict() method. Some models, like doubleml.DoubleMLIRM and doubleml.DoubleMLIIVM require classifiers.

In case of classifiers, the learner needs to come with a predict_proba() instead of, or in addition to, a predict() method, see for example sklearn.ensemble.RandomForestClassifier.predict_proba().

In order to be able to use the set_ml_nuisance_params() method of DoubleML classes the learner additionally needs to come with a set_params() method, see for example sklearn.ensemble.RandomForestRegressor.set_params().

We further rely on the function sklearn.base.clone() which adds the requirement of a get_params() method for a learner in order to be used for nuisance models of DoubleML model classes.

Most learners from scikit-learn satisfy all these minimum requirements.

7.1.2. Specifying learners and set hyperparameters
The learners are set during initialization of the DoubleML model classes doubleml.DoubleMLPLR, doubleml.DoubleMLPLIV, doubleml.DoubleMLIRM and doubleml.DoubleMLIIVM. Lets simulate some data and consider the partially linear regression model. We need to specify learners for the nuisance functions 
 and 
, for example sklearn.ensemble.RandomForestRegressor.


Python
import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

np.random.seed(1234)

ml_l = RandomForestRegressor()

ml_m = RandomForestRegressor()

data = make_plr_CCDDHNR2018(alpha=0.5, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

dml_plr_obj.fit().summary
Out[10]: 
       coef   std err          t         P>|t|    2.5 %    97.5 %
d  0.503504  0.045993  10.947466  6.833227e-28  0.41336  0.593648
Without further specification of the hyperparameters, default values are used. To set hyperparameters:

We can also use pre-parametrized learners, like RandomForestRegressor(n_estimators=10).

Alternatively, hyperparameters can also be set after initialization via the method set_ml_nuisance_params(learner, treat_var, params)


Python
np.random.seed(1234)

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data,
                              RandomForestRegressor(n_estimators=10),
                              RandomForestRegressor())


print(dml_plr_obj.fit().summary)
      coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.53257  0.046922  11.350165  7.402301e-30  0.440605  0.624535

np.random.seed(1234)

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data,
                              RandomForestRegressor(),
                              RandomForestRegressor())


dml_plr_obj.set_ml_nuisance_params('ml_l', 'd', {'n_estimators': 10});

print(dml_plr_obj.fit().summary)
      coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.53257  0.046922  11.350165  7.402301e-30  0.440605  0.624535
Setting treatment-variable-specific or fold-specific hyperparameters:

In the multiple-treatment case, the method set_ml_nuisance_params(learner, treat_var, params) can be used to set different hyperparameters for different treatment variables.

The method set_ml_nuisance_params(learner, treat_var, params) accepts dicts and lists for params. A dict should be provided if for each fold the same hyperparameters should be used. Fold-specific parameters are supported. To do so, provide a nested list as params, where the outer list is of length n_rep and the inner list of length n_folds.

7.1.3. Hyperparameter tuning
Parameter tuning of learners for the nuisance functions of DoubleML models can be done via the tune() method. To illustrate the parameter tuning, we generate data from a sparse partially linear regression model.


Python
import doubleml as dml

import numpy as np

np.random.seed(3141)

n_obs = 200

n_vars = 200

theta = 3

X = np.random.normal(size=(n_obs, n_vars))

d = np.dot(X[:, :3], np.array([5, 5, 5])) + np.random.standard_normal(size=(n_obs,))

y = theta * d + np.dot(X[:, :3], np.array([5, 5, 5])) + np.random.standard_normal(size=(n_obs,))

dml_data = dml.DoubleMLData.from_arrays(X, y, d)
The hyperparameter-tuning is performed using either an exhaustive search over specified parameter values implemented in sklearn.model_selection.GridSearchCV or via a randomized search implemented in sklearn.model_selection.RandomizedSearchCV.


Python
import doubleml as dml

from sklearn.linear_model import Lasso

ml_l = Lasso()

ml_m = Lasso()

dml_plr_obj = dml.DoubleMLPLR(dml_data, ml_l, ml_m)

par_grids = {'ml_l': {'alpha': np.arange(0.05, 1., 0.1)},
             'ml_m': {'alpha': np.arange(0.05, 1., 0.1)}}


dml_plr_obj.tune(par_grids, search_mode='grid_search');

print(dml_plr_obj.params)
{'ml_l': {'d': [[{'alpha': np.float64(0.45000000000000007)}, {'alpha': np.float64(0.45000000000000007)}, {'alpha': np.float64(0.45000000000000007)}, {'alpha': np.float64(0.45000000000000007)}, {'alpha': np.float64(0.45000000000000007)}]]}, 'ml_m': {'d': [[{'alpha': np.float64(0.15000000000000002)}, {'alpha': np.float64(0.15000000000000002)}, {'alpha': np.float64(0.15000000000000002)}, {'alpha': np.float64(0.15000000000000002)}, {'alpha': np.float64(0.15000000000000002)}]]}}

print(dml_plr_obj.fit().summary)
       coef   std err          t  P>|t|     2.5 %    97.5 %
d  3.031134  0.071777  42.229759    0.0  2.890454  3.171815

np.random.seed(1234)

par_grids = {'ml_l': {'alpha': np.arange(0.05, 1., 0.01)},
             'ml_m': {'alpha': np.arange(0.05, 1., 0.01)}}


dml_plr_obj.tune(par_grids, search_mode='randomized_search', n_iter_randomized_search=20);

print(dml_plr_obj.params)
{'ml_l': {'d': [[{'alpha': np.float64(0.4000000000000001)}, {'alpha': np.float64(0.4000000000000001)}, {'alpha': np.float64(0.4000000000000001)}, {'alpha': np.float64(0.4000000000000001)}, {'alpha': np.float64(0.4000000000000001)}]]}, 'ml_m': {'d': [[{'alpha': np.float64(0.09000000000000001)}, {'alpha': np.float64(0.09000000000000001)}, {'alpha': np.float64(0.09000000000000001)}, {'alpha': np.float64(0.09000000000000001)}, {'alpha': np.float64(0.09000000000000001)}]]}}

print(dml_plr_obj.fit().summary)
      coef   std err          t          P>|t|     2.5 %    97.5 %
d  2.96582  0.086679  34.216207  1.388216e-256  2.795932  3.135707
Hyperparameter tuning can also be done with more sophisticated methods, like for example an iterative fitting along a regularization path implemented in sklearn.linear_model.LassoCV. In this case the tuning should be done externally and the parameters can then be set via the set_ml_nuisance_params() method.


Python
import doubleml as dml

from sklearn.linear_model import LassoCV

np.random.seed(1234)

ml_l_tune = LassoCV().fit(dml_data.x, dml_data.y)

ml_m_tune = LassoCV().fit(dml_data.x, dml_data.d)

ml_l = Lasso()

ml_m = Lasso()

dml_plr_obj = dml.DoubleMLPLR(dml_data, ml_l, ml_m)

dml_plr_obj.set_ml_nuisance_params('ml_l', 'd', {'alpha': ml_l_tune.alpha_});

dml_plr_obj.set_ml_nuisance_params('ml_m', 'd', {'alpha': ml_m_tune.alpha_});

print(dml_plr_obj.params)
{'ml_l': {'d': [[{'alpha': np.float64(0.4311947070055128)}, {'alpha': np.float64(0.4311947070055128)}, {'alpha': np.float64(0.4311947070055128)}, {'alpha': np.float64(0.4311947070055128)}, {'alpha': np.float64(0.4311947070055128)}]]}, 'ml_m': {'d': [[{'alpha': np.float64(0.14281403493938022)}, {'alpha': np.float64(0.14281403493938022)}, {'alpha': np.float64(0.14281403493938022)}, {'alpha': np.float64(0.14281403493938022)}, {'alpha': np.float64(0.14281403493938022)}]]}}

print(dml_plr_obj.fit().summary)
       coef   std err          t  P>|t|     2.5 %    97.5 %
d  3.048723  0.075869  40.183855    0.0  2.900021  3.197424
7.1.4. Evaluate learners
To compare different learners it is possible to evaluate the out-of-sample performance of each learner. The summary already displays either the root-mean-squared error (for regressions) or log-loss (for classifications) for each learner and each corresponding repetition of cross-fitting (n_rep argument).

To illustrate the parameter tuning, we work with the following example.


Python
import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

np.random.seed(1234)

ml_l = RandomForestRegressor()

ml_m = RandomForestRegressor()

data = make_plr_CCDDHNR2018(alpha=0.5, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

dml_plr_obj.fit()
Out[63]: <doubleml.plm.plr.DoubleMLPLR at 0x7fccdbccb6b0>

print(dml_plr_obj)
================== DoubleMLPLR Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: partialling out

------------------ Machine learner   ------------------
Learner ml_l: RandomForestRegressor()
Learner ml_m: RandomForestRegressor()
Out-of-sample Performance:
Regression:
Learner ml_l RMSE: [[1.17385178]]
Learner ml_m RMSE: [[1.03244552]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err          t         P>|t|    2.5 %    97.5 %
d  0.503504  0.045993  10.947466  6.833227e-28  0.41336  0.593648
The loss of each learner are also stored in the nuisance_loss attribute. Further, the evaluate_learners() method allows to evalute customized evaluation metrics as e.g. the mean absolute error. The default option is still the root-mean-squared error for evaluation.


Python
print(dml_plr_obj.nuisance_loss)
{'ml_l': array([[1.17385178]]), 'ml_m': array([[1.03244552]])}

print(dml_plr_obj.evaluate_learners())
{'ml_l': array([[1.17385178]]), 'ml_m': array([[1.03244552]])}
To evaluate a customized metric one has to define a callable. For some models (e.g. the IRM model) it is important that the metric can handle nan values as not all target values are known.


Python
from sklearn.metrics import mean_absolute_error

def mae(y_true, y_pred):
    subset = np.logical_not(np.isnan(y_true))
    return mean_absolute_error(y_true[subset], y_pred[subset])


dml_plr_obj.evaluate_learners(learners=['ml_l'], metric=mae)
Out[69]: {'ml_l': array([[0.95559917]])}
A more detailed notebook on the choice of learners is available in the example gallery.

7.1.5. Advanced: External Predictions
Since there might be cases where the user wants to use a learner that is not supported by DoubleML or do some extensive hyperparameter tuning, it is possible to use external predictions for the nuisance functions. Remark that this requires the user to take care of the cross-fitting procedure and learner evaluation.

To illustrate the use of external predictions, we work with the following example.


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_irm_data

from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

np.random.seed(3333)

data = make_irm_data(theta=0.5, n_obs=500, dim_x=10, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

# DoubleML with interal predictions
ml_g = RandomForestRegressor(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

ml_m = RandomForestClassifier(n_estimators=100, max_features=10, max_depth=5, min_samples_leaf=2)

dml_irm_obj = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

dml_irm_obj.fit()
Out[80]: <doubleml.irm.irm.DoubleMLIRM at 0x7fcce9a0c9e0>

print(dml_irm_obj.summary)
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.599297   0.1887  3.175931  0.001494  0.229452  0.969141
The doubleml.DoubleMLIRM model class saves nuisance predictions in the predictions attribute as a nested dictionary. To rely on external predictions, the user has to provide a nested dictionary, where the outer level keys correspond to the treatment variable names and the inner level keys correspond to the nuisance learner names. Further the values have to be numpy arrays of shape (n_obs, n_rep). Here we generate an external predictions dictionary from the internal predictions attribute.


Python
pred_dict = {"d": {
    "ml_g0": dml_irm_obj.predictions["ml_g0"][:, :, 0],
    "ml_g1": dml_irm_obj.predictions["ml_g1"][:, :, 0],
    "ml_m": dml_irm_obj.predictions["ml_m"][:, :, 0]
    }
}

The external predictions can be passed to the fit() method of the doubleml.DoubleML class via the external_predictions argument.


Python
ml_g = dml.utils.DMLDummyRegressor()

ml_m = dml.utils.DMLDummyClassifier()

dml_irm_obj_ext = dml.DoubleMLIRM(obj_dml_data, ml_g, ml_m)

dml_irm_obj_ext.fit(external_predictions=pred_dict)
Out[86]: <doubleml.irm.irm.DoubleMLIRM at 0x7fcce9a0e780>

print(dml_irm_obj_ext.summary)
       coef  std err         t     P>|t|     2.5 %    97.5 %
d  0.599297   0.1887  3.175931  0.001494  0.229452  0.969141
Both model have identical estimates. Remark that doubleml.DoubleML class usually require learners for initialization. With external predictions these learners are not used. The DMLDummyRegressor and DMLDummyClassifier are dummy learners which are used to initialize the doubleml.DoubleML class. Both dummy learners raise errors if specific methods are called to safeguard against undesired behavior. Further, the doubleml.DoubleMLData class requires features (e.g. via the x_cols argument) which are not used. This can be handled by adding a dummy column to the data.

7.2. R: Learners and hyperparameters
7.2.1. Minimum requirements for learners
The minimum requirement for a learner to be used for nuisance models in the DoubleML package is

The implementation as a learner for regression or classification in the mlr3 package or its extension packages mlr3learners and mlr3extralearners . A guide on how to add a learner is provided in the chapter on extending learners in the mlr3 book .

The mlr3 package makes sure that the learners satisfy some core functionalities. To specify a specific learner in DoubleML users can pass objects of the class Learner. A fast way to construct these objects is to use the mlr3 function lrn(). An introduction to learners in mlr3 is provided in the chapter on learners of the mlr3 book.

It is also possible to pass learners that have been constructed from a pipeline with the mlr3pipelines package.

The models DoubleML::DoubleMLIRM and DoubleML::DoubleMLIIVM require classifiers. Users can also specify classifiers in the DoubleML::DoubleMLPLR in cases with binary treatment variables.

Hyperparameters of learners can either be set at instantiation in mlr3 or after instantiation using the set_ml_nuisance_params() method.

An interactive list of provided learners in the mlr3 and extension packages can be found on the website of the mlr3extralearners package.

7.2.2. Specifying learners and set hyperparameters
The learners are set during initialization of the DoubleML model classes DoubleML::DoubleMLPLR, DoubleML::DoubleMLPLIV , DoubleML::DoubleMLIRM and DoubleML::DoubleMLIIVM. Lets simulate some data and consider the partially linear regression model. We need to specify learners for the nuisance functions 
 and 
, for example LearnerRegrRanger (lrn("regr.ranger")) for regression with random forests based on the ranger package for R.


R
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(data.table)
lgr::get_logger("mlr3")$set_threshold("warn")

# set up a mlr3 learner
learner = lrn("regr.ranger")
ml_l = learner$clone()
ml_m = learner$clone()
set.seed(3141)
data = make_plr_CCDDHNR2018(alpha=0.5, return_type='data.table')
obj_dml_data = DoubleMLData$new(data, y_col="y", d_cols="d")
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)
dml_plr_obj$fit()
dml_plr_obj$summary()
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d    0.5748     0.0445   12.92   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Without further specification of the hyperparameters, default values are used. To set hyperparameters:

We can also use pre-parametrized learners lrn("regr.ranger", num.trees=10).

Alternatively, hyperparameters can be set after initialization via the method set_ml_nuisance_params(learner, treat_var, params, set_fold_specific).


R
set.seed(3141)
ml_l = lrn("regr.ranger", num.trees=10)
ml_m = lrn("regr.ranger")
obj_dml_data = DoubleMLData$new(data, y_col="y", d_cols="d")
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)
dml_plr_obj$fit()
dml_plr_obj$summary()

set.seed(3141)
ml_l = lrn("regr.ranger")
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l , ml_m)
dml_plr_obj$set_ml_nuisance_params("ml_l", "d", list("num.trees"=10))
dml_plr_obj$fit()
dml_plr_obj$summary()
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.58812    0.04502   13.06   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.58812    0.04502   13.06   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Setting treatment-variable-specific or fold-specific hyperparameters:

In the multiple-treatment case, the method set_ml_nuisance_params(learner, treat_var, params, set_fold_specific) can be used to set different hyperparameters for different treatment variables.

The method set_ml_nuisance_params(learner, treat_var, params, set_fold_specific) accepts lists for params. The structure of the list depends on whether the same parameters should be provided for all folds or separate values are passed for specific folds.

Global parameter passing: The values in params are used for estimation on all folds. The named list in the argument params should have entries with names corresponding to the parameters of the learners. It is required that option set_fold_specific is set to FALSE (default).

Fold-specific parameter passing: params is a nested list. The outer list needs to be of length n_rep and the inner list of length n_folds. The innermost list must have named entries that correspond to the parameters of the learner. It is required that option set_fold_specific is set to TRUE. Moreover, fold-specific parameter passing is only supported, if all parameters are set fold-specific.

External setting of parameters will override previously set parameters. To assert the choice of parameters, access the fields $learner and $params.


R
set.seed(3141)
ml_l = lrn("regr.ranger")
ml_m = lrn("regr.ranger")
obj_dml_data = DoubleMLData$new(data, y_col="y", d_cols="d")

n_rep = 2
n_folds = 3
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m, n_rep=n_rep, n_folds=n_folds)

# Set globally
params = list("num.trees"=10)
dml_plr_obj$set_ml_nuisance_params("ml_l", "d", params=params)
dml_plr_obj$set_ml_nuisance_params("ml_m", "d", params=params)
dml_plr_obj$learner
dml_plr_obj$params
dml_plr_obj$fit()
dml_plr_obj$summary()
$ml_l

[36m──[39m [1m[34m<LearnerRegrRanger>[39m (regr.ranger): Random Forest[22m [36m────────────────────────────[39m
• Model: -
• Parameters: num.threads=1
• Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, and quantiles
• Feature Types: logical, integer, numeric, character, factor, and ordered
• Encapsulation: none (fallback: -)
• Properties: hotstart_backward, importance, missings, oob_error,
selected_features, and weights
• Other settings: use_weights = 'use'

$ml_m

[36m──[39m [1m[34m<LearnerRegrRanger>[39m (regr.ranger): Random Forest[22m [36m────────────────────────────[39m
• Model: -
• Parameters: num.threads=1
• Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, and quantiles
• Feature Types: logical, integer, numeric, character, factor, and ordered
• Encapsulation: none (fallback: -)
• Properties: hotstart_backward, importance, missings, oob_error,
selected_features, and weights
• Other settings: use_weights = 'use'
$ml_l
$d = $num.trees = 10
$ml_m
$d = $num.trees = 10
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.52732    0.04586    11.5   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


The following example illustrates how to set parameters for each fold.


R
learner = lrn("regr.ranger")
ml_l = learner$clone()
ml_m = learner$clone()
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m, n_rep=n_rep, n_folds=n_folds)

# Set values for each fold
params_exact = rep(list(rep(list(params), n_folds)), n_rep)
dml_plr_obj$set_ml_nuisance_params("ml_l", "d", params=params_exact,
                                     set_fold_specific=TRUE)
dml_plr_obj$set_ml_nuisance_params("ml_m", "d", params=params_exact,
                                     set_fold_specific=TRUE)
dml_plr_obj$learner
dml_plr_obj$params
dml_plr_obj$fit()
dml_plr_obj$summary()
$ml_l

[36m──[39m [1m[34m<LearnerRegrRanger>[39m (regr.ranger): Random Forest[22m [36m────────────────────────────[39m
• Model: -
• Parameters: num.threads=1
• Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, and quantiles
• Feature Types: logical, integer, numeric, character, factor, and ordered
• Encapsulation: none (fallback: -)
• Properties: hotstart_backward, importance, missings, oob_error,
selected_features, and weights
• Other settings: use_weights = 'use'

$ml_m

[36m──[39m [1m[34m<LearnerRegrRanger>[39m (regr.ranger): Random Forest[22m [36m────────────────────────────[39m
• Model: -
• Parameters: num.threads=1
• Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, and quantiles
• Feature Types: logical, integer, numeric, character, factor, and ordered
• Encapsulation: none (fallback: -)
• Properties: hotstart_backward, importance, missings, oob_error,
selected_features, and weights
• Other settings: use_weights = 'use'
$ml_l
$d =
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
$ml_m
$d =
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
$num.trees = 10
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.49693    0.04387   11.33   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


7.2.3. Using pipelines to construct learners
Users can also specify learners that have been constructed from a pipeline using the mlr3pipelines package. In general, pipelines can be used to perform data preprocessing, feature selection, combine learners and even to perform hyperparameter tuning. In the following, we provide two examples on how to construct a single learner and how to stack different learners via a pipeline. For a more detailed introduction to mlr3pipelines, we refer to the Pipelines Chapter in the mlr3book. Moreover, a notebook on how to use mlr3pipelines in combination with DoubleML is available in the example gallery.


R
library(mlr3pipelines)

set.seed(3141)
# Define random forest learner in a pipeline
single_learner_pipeline = po("learner", lrn("regr.ranger", num.trees = 10))

# Use pipeline to create a new instance of a learner
ml_g = as_learner(single_learner_pipeline)
ml_m = as_learner(single_learner_pipeline)

obj_dml_data = DoubleMLData$new(data, y_col="y", d_cols="d")

n_rep = 2
n_folds = 3
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_g, ml_m, n_rep=n_rep, n_folds=n_folds)
dml_plr_obj$learner
dml_plr_obj$fit()
dml_plr_obj$summary()

set.seed(3141)
# Define ensemble learner in a pipeline
ensemble_learner_pipeline = gunion(list(
        po("learner", lrn("regr.cv_glmnet", s = "lambda.min")),
        po("learner", lrn("regr.ranger")),
        po("learner", lrn("regr.rpart", cp = 0.01)))) %>>%
    po("regravg", 3)

# Use pipeline to create a new instance of a learner
ml_g = as_learner(ensemble_learner_pipeline)
ml_m = as_learner(ensemble_learner_pipeline)

obj_dml_data = DoubleMLData$new(data, y_col="y", d_cols="d")

n_rep = 2
n_folds = 3
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_g, ml_m, n_rep=n_rep, n_folds=n_folds)
dml_plr_obj$learner
dml_plr_obj$fit()
dml_plr_obj$summary()
$ml_l

[36m──[39m [1m[34m<GraphLearner>[39m (regr.ranger)[22m [36m────────────────────────────────────────────────[39m
• Model: -
• Parameters: regr.ranger.num.threads=1, regr.ranger.num.trees=10
• Validate: [34m<NULL>[39m
• Packages: [34mmlr3[39m, [34mmlr3pipelines[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, quantiles, and distr
• Feature Types: logical, integer, numeric, character, factor, ordered,
POSIXct, and Date
• Encapsulation: none (fallback: -)
• Properties: featureless, hotstart_backward, hotstart_forward, importance,
marshal, missings, offset, oob_error, selected_features, and weights
• Other settings: use_weights = 'use'

$ml_m

[36m──[39m [1m[34m<GraphLearner>[39m (regr.ranger)[22m [36m────────────────────────────────────────────────[39m
• Model: -
• Parameters: regr.ranger.num.threads=1, regr.ranger.num.trees=10
• Validate: [34m<NULL>[39m
• Packages: [34mmlr3[39m, [34mmlr3pipelines[39m, [34mmlr3learners[39m, and [34mranger[39m
• Predict Types: [response], se, quantiles, and distr
• Feature Types: logical, integer, numeric, character, factor, ordered,
POSIXct, and Date
• Encapsulation: none (fallback: -)
• Properties: featureless, hotstart_backward, hotstart_forward, importance,
marshal, missings, offset, oob_error, selected_features, and weights
• Other settings: use_weights = 'use'
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.52732    0.04586    11.5   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


$ml_l

[36m──[39m [1m[34m<GraphLearner>[39m (regr.cv_glmnet.regr.ranger.regr.rpart.regravg)[22m [36m──────────────[39m
• Model: -
• Parameters: regr.cv_glmnet.family=gaussian,
regr.cv_glmnet.use_pred_offset=TRUE, regr.cv_glmnet.s=lambda.min,
regr.ranger.num.threads=1, regr.rpart.cp=0.01, regr.rpart.xval=0,
regravg.weights=1
• Validate: [34m<NULL>[39m
• Packages: [34mmlr3[39m, [34mmlr3pipelines[39m, [34mmlr3learners[39m, [34mglmnet[39m, [34mranger[39m, and [34mrpart[39m
• Predict Types: [response], se, quantiles, and distr
• Feature Types: logical, integer, numeric, character, factor, ordered,
POSIXct, and Date
• Encapsulation: none (fallback: -)
• Properties: featureless, hotstart_backward, hotstart_forward, importance,
marshal, missings, offset, oob_error, selected_features, and weights
• Other settings: use_weights = 'use'

$ml_m

[36m──[39m [1m[34m<GraphLearner>[39m (regr.cv_glmnet.regr.ranger.regr.rpart.regravg)[22m [36m──────────────[39m
• Model: -
• Parameters: regr.cv_glmnet.family=gaussian,
regr.cv_glmnet.use_pred_offset=TRUE, regr.cv_glmnet.s=lambda.min,
regr.ranger.num.threads=1, regr.rpart.cp=0.01, regr.rpart.xval=0,
regravg.weights=1
• Validate: [34m<NULL>[39m
• Packages: [34mmlr3[39m, [34mmlr3pipelines[39m, [34mmlr3learners[39m, [34mglmnet[39m, [34mranger[39m, and [34mrpart[39m
• Predict Types: [response], se, quantiles, and distr
• Feature Types: logical, integer, numeric, character, factor, ordered,
POSIXct, and Date
• Encapsulation: none (fallback: -)
• Properties: featureless, hotstart_backward, hotstart_forward, importance,
marshal, missings, offset, oob_error, selected_features, and weights
• Other settings: use_weights = 'use'
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.55176    0.04625   11.93   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


7.2.4. Hyperparameter tuning
Parameter tuning of learners for the nuisance functions of DoubleML models can be done via the tune() method. The tune() method passes various options and parameters to the tuning interface provided by the mlr3tuning package. The mlr3 book provides a step-by-step introduction to parameter tuning.

To illustrate the parameter tuning, we generate data from a sparse partially linear regression model.


R
library(DoubleML)
library(mlr3)
library(data.table)

set.seed(3141)
n_obs = 200
n_vars = 200
theta = 3
X = matrix(stats::rnorm(n_obs * n_vars), nrow = n_obs, ncol = n_vars)
d = X[, 1:3, drop = FALSE] %*% c(5, 5, 5) + stats::rnorm(n_obs)
y = theta * d + X[, 1:3, drop = FALSE] %*% c(5, 5, 5)  + stats::rnorm(n_obs)
dml_data = double_ml_data_from_matrix(X = X, y = y, d = d)
The hyperparameter-tuning is performed according to options passed through a named list tune_settings. The entries in the list specify options during parameter tuning with mlr3tuning:

terminator is a Terminator object passed to mlr3tuning that manages the budget to solve the tuning problem.

algorithm is an object of class Tuner and specifies the tuning algorithm. Alternatively, algorithm can be a character() that is used as an argument in the wrapper mlr3tuning call tnr(algorithm). The corresponding chapter in the mlr3book illustrates how the Tuner class supports grid search, random search, generalized simulated annealing and non-linear optimization.

rsmp_tune is an object of class mlr3 resampling that specifies the resampling method for evaluation, for example rsmp(“cv”, folds = 5) implements 5-fold cross-validation. rsmp(“holdout”, ratio = 0.8) implements an evaluation based on a hold-out sample that contains 20 percent of the observations. By default, 5-fold cross-validation is performed.

measure is a named list containing the measures used for tuning of the nuisance components. The names of the entries must match the learner names (see method learner_names()). The entries in the list must either be objects of class Measure or keys passed to msr(). If measure is not provided by the user, default measures are used, i.e., mean squared error for regression models and classification error for binary outcomes.

In the following example, we tune the penalty parameter 
 (lambda) for lasso with the R package glmnet. To tune the value of lambda, a grid search is performed over a grid of values that range from 0.05 to 0.1 at a resolution of 10. Using a resolution of 10 splits the grid of values in 10 equally spaced values ranging from a minimum of 0.05 to a maximum of 0.1. To evaluate the predictive performance in both nuisance parts, the cross-validated mean squared error is used.

Setting the option tune_on_folds=FALSE, the tuning is performed on the whole sample. Hence, the cross-validated errors are obtained from a random split of the whole sample into 5 folds. As a result, one set of lambda values are obtained which are later used in the fitting stage for all folds.

Alternatively, setting the option tune_on_folds=TRUE would assign the tuning resampling scheme rsmp_tune to each fold. For example, if we set n_folds=2 at initialization of the DoubleMLPLR object and use a 5-fold cross-validated error for tuning, each of the two folds would be split up into 5 subfolds and the error would be evaluated on these subfolds.


R
library(DoubleML)
library(mlr3)
library(data.table)
library(mlr3learners)
library(mlr3tuning)
library(paradox)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

set.seed(1234)
ml_l = lrn("regr.glmnet")
ml_m = lrn("regr.glmnet")
dml_plr_obj = DoubleMLPLR$new(dml_data, ml_l, ml_m)

par_grids = list(
  "ml_l" = ps(lambda = p_dbl(lower = 0.05, upper = 0.1)),
  "ml_m" = ps(lambda = p_dbl(lower = 0.05, upper = 0.1)))

tune_settings = list(terminator = trm("evals", n_evals = 100),
                      algorithm = tnr("grid_search", resolution = 10),
                      rsmp_tune = rsmp("cv", folds = 5),
                      measure = list("ml_l" = msr("regr.mse"),
                                     "ml_m" = msr("regr.mse")))
dml_plr_obj$tune(param_set=par_grids, tune_settings=tune_settings, tune_on_fold=TRUE)
dml_plr_obj$params

dml_plr_obj$fit()
dml_plr_obj$summary()
$ml_l
$d =
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.0777777777777778
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$ml_m
$d =
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
$family
'gaussian'
$use_pred_offset
TRUE
$lambda
0.1
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d    3.0425     0.1424   21.37   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


Hyperparameter tuning can also be done with more sophisticated methods, for example by using built-in tuning paths of learners. For example, the learner regr.cv_glmnet performs an internal cross-validated choice of the parameter lambda. Alternatively, the powerful functionalities of the mlr3tuning package can be used for external parameter tuning of the nuisance parts. The optimally chosen parameters can then be passed to the DoubleML models using the set_ml_nuisance_params() method.


R
library(DoubleML)
library(mlr3)
library(data.table)
library(mlr3learners)
library(mlr3tuning)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

set.seed(1234)
ml_l = lrn("regr.cv_glmnet", s="lambda.min")
ml_m = lrn("regr.cv_glmnet", s="lambda.min")
dml_plr_obj = DoubleMLPLR$new(dml_data, ml_l, ml_m)

dml_plr_obj$fit()
dml_plr_obj$summary()
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   3.08848    0.07366   41.93   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


The following code chunk illustrates another example for global parameter tuning with random forests as provided by the ranger package. In this example, we use random search to find optimal parameters mtry and max.depth of a random forest. Evaluation is based on 3-fold cross-validation.


R
library(DoubleML)
library(mlr3)
library(mlr3learners)
library(data.table)
library(mlr3tuning)
library(paradox)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

# set up a mlr3 learner
learner = lrn("regr.ranger")
ml_l = learner$clone()
ml_m = learner$clone()

set.seed(3141)
obj_dml_data = make_plr_CCDDHNR2018(alpha=0.5)
dml_plr_obj = DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)

# set up a list of parameter grids
param_grid = list("ml_l" = ps(mtry = p_int(lower = 2 , upper = 20),
                              max.depth = p_int(lower = 2, upper = 5)),
                  "ml_m" = ps(mtry = p_int(lower = 2 , upper = 20),
                              max.depth = p_int(lower = 2, upper = 5)))

tune_settings = list(terminator = mlr3tuning::trm("evals", n_evals = 20),
                      algorithm = tnr("random_search"),
                      rsmp_tune = rsmp("cv", folds = 3),
                      measure = list("ml_l" = msr("regr.mse"),
                                     "ml_m" = msr("regr.mse")))
dml_plr_obj$tune(param_set=param_grid, tune_settings=tune_settings, tune_on_folds=FALSE)
dml_plr_obj$params

dml_plr_obj$fit()
dml_plr_obj$summary()
$ml_l
$d =
$num.threads
1
$mtry
10
$max.depth
5
$ml_m
$d =
$num.threads
1
$mtry
17
$max.depth
3
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d   0.55307    0.04563   12.12   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


7.2.5. Hyperparameter tuning with pipelines
As an alternative to the previously presented tuning approach, it is possible to base the parameter tuning on a pipeline as provided by the mlr3pipelines package. The basic idea of this approach is to define a learner via a pipeline and then perform the tuning via the tune(). We will shortly repeat the lasso example from above. In general, the pipeline-based approach can be used to find optimal values not only for the parameters of one or multiple learners, but also for other parameters, which are, for example, involved in the data preprocessing. We refer to more details provided in the Pipelines Chapter in the mlr3book.


R
library(DoubleML)
library(mlr3)
library(mlr3tuning)
library(mlr3pipelines)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

# Define learner in a pipeline
set.seed(1234)
lasso_pipe = po("learner",
    learner = lrn("regr.glmnet"))
ml_g = as_learner(lasso_pipe)
ml_m = as_learner(lasso_pipe)

# Instantiate a DoubleML object
dml_plr_obj = DoubleMLPLR$new(dml_data, ml_g, ml_m)

# Parameter grid for lambda
par_grids = ps(regr.glmnet.lambda = p_dbl(lower = 0.05, upper = 0.1))

tune_settings = list(terminator = trm("evals", n_evals = 100),
                     algorithm = tnr("grid_search", resolution = 10),
                     rsmp_tune = rsmp("cv", folds = 5),
                     measure = list("ml_g" = msr("regr.mse"),
                                    "ml_m" = msr("regr.mse")))
dml_plr_obj$tune(param_set = list("ml_g" = par_grids,
                                  "ml_m" = par_grids),
                                  tune_settings=tune_settings,
                                  tune_on_fold=TRUE)
dml_plr_obj$fit()
dml_plr_obj$summary()
Estimates and significance testing of the effect of target variables
  Estimate. Std. Error t value Pr(>|t|)    
d    3.0425     0.1424   21.37   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


7.2.5.1. References
Lang, M., Binder, M., Richter, J., Schratz, P., Pfisterer, F., Coors, S., Au, Q., Casalicchio, G., Kotthoff, L., Bischl, B. (2019), mlr3: A modern object-oriented machine learing framework in R. Journal of Open Source Software, doi:10.21105/joss.01903.

Becker, M., Binder, M., Bischl, B., Lang, M., Pfisterer, F., Reich, N.G., Richter, J., Schratz, P., Sonabend, R. (2020), mlr3 book, available at https://mlr3book.mlr-org.com.

previous

6. Double machine learning algorithms

next

8. Variance estimation and confidence intervals

 On this page
7.1. Python: Learners and hyperparameters
7.1.1. Minimum requirements for learners
7.1.2. Specifying learners and set hyperparameters
7.1.3. Hyperparameter tuning
7.1.4. Evaluate learners
7.1.5. Advanced: External Predictions
7.2. R: Learners and hyperparameters
7.2.1. Minimum requirements for learners
7.2.2. Specifying learners and set hyperparameters
7.2.3. Using pipelines to construct learners
7.2.4. Hyperparameter tuning
7.2.5. Hyperparameter tuning with pipelines
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
8. Variance estimation and confidence intervals
8.1. Variance estimation
Under regularity conditions the estimator 
 concentrates in a 
-neighborhood of 
 and the sampling error 
 is approximately normal

with mean zero and variance given by

where 
, if the score function is linear in the parameter 
. If the score is not linear in the parameter 
, then 
.

Estimates of the variance are obtained by

 
 
 
 
 
 
 
 
 
for score functions being linear in the parameter 
. For non-linear score functions, the implementation assumes that derivatives and expectations are interchangeable, so that

 
 
 
An approximate confidence interval is given by

As an example we consider a partially linear regression model (PLR) implemented in DoubleMLPLR.


Python
import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

np.random.seed(3141)

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

data = make_plr_CCDDHNR2018(alpha=0.5, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)

dml_plr_obj.fit();

R
The fit() method of DoubleMLPLR stores the estimate 
 in its coef attribute.


Python
print(dml_plr_obj.coef)
[0.48069071]

R
The asymptotic standard error 
 is stored in its se attribute.


Python
print(dml_plr_obj.se)
[0.04053339]

R
Additionally, the value of the 
-statistic and the corresponding p-value are provided in the attributes t_stat and pval.


Python
print(dml_plr_obj.t_stat)
[11.85912862]

print(dml_plr_obj.pval)
[1.92972925e-32]

R
Note

In Python, an overview of all these estimates, together with a 95 % confidence interval is stored in the attribute summary.

In R, a summary can be obtained by using the method summary(). The confint() method performs estimation of confidence intervals.


Python
print(dml_plr_obj.summary)
       coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.480691  0.040533  11.859129  1.929729e-32  0.401247  0.560135

R
A more detailed overview of the fitted model, its specifications and the summary can be obtained via the string-representation of the object.


Python
print(dml_plr_obj)
================== DoubleMLPLR Object ==================

------------------ Data summary      ------------------
Outcome variable: y
Treatment variable(s): ['d']
Covariates: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']
Instrument variable(s): None
No. Observations: 500

------------------ Score & algorithm ------------------
Score function: partialling out

------------------ Machine learner   ------------------
Learner ml_l: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Learner ml_m: RandomForestRegressor(max_depth=5, max_features=20, min_samples_leaf=2)
Out-of-sample Performance:
Regression:
Learner ml_l RMSE: [[1.12705095]]
Learner ml_m RMSE: [[1.03917696]]

------------------ Resampling        ------------------
No. folds: 5
No. repeated sample splits: 1

------------------ Fit summary       ------------------
       coef   std err          t         P>|t|     2.5 %    97.5 %
d  0.480691  0.040533  11.859129  1.929729e-32  0.401247  0.560135

R
8.2. Confidence bands and multiplier bootstrap for valid simultaneous inference
DoubleML provides methods to perform valid simultaneous inference for multiple treatment variables. As an example, consider a PLR with 
 causal parameters of interest 
 associated with treatment variables 
. Inference on multiple target coefficients can be performed by iteratively applying the DML inference procedure over the target variables of interests: Each of the coefficients of interest, 
, with 
, solves a corresponding moment condition

Analogously to the case with a single parameter of interest, the PLR model with multiple treatment variables includes two regression steps to achieve orthogonality. First, the main regression is given by

with 
 being a matrix comprising the confounders, 
, and all remaining treatment variables 
 with 
, by default. Second, the relationship between the treatment variable 
 and the remaining explanatory variables is determined by the equation

For further details, we refer to Belloni et al. (2018). Simultaneous inference can be based on a multiplier bootstrap procedure introduced in Chernozhukov et al. (2013, 2014). Alternatively, traditional correction approaches, for example the Bonferroni correction, can be used to adjust p-values.

The bootstrap() method provides an implementation of a multiplier bootstrap for double machine learning models. For 
 weights 
 are generated according to a normal (Gaussian) bootstrap, wild bootstrap or exponential bootstrap. The number of bootstrap samples is provided as input n_rep_boot and for method one can choose 'Bayes', 'normal' or 'wild'. Based on the estimates of the standard errors 
 and 
 that are obtained from DML, we construct bootstraped t-statistics 
 for 

 
 
 
The output of the multiplier bootstrap can be used to determine the constant, 
 that is required for the construction of a simultaneous 
 confidence band

To demonstrate the bootstrap, we simulate data from a sparse partially linear regression model. Then we estimate the PLR model and perform the multiplier bootstrap. Joint confidence intervals based on the multiplier bootstrap are then obtained by setting the option joint when calling the method confint.

Moreover, a multiple hypotheses testing adjustment of p-values from a high-dimensional model can be obtained with the method p_adjust. DoubleML performs a version of the Romano-Wolf stepdown adjustment, which is based on the multiplier bootstrap, by default. Alternatively, p_adjust allows users to apply traditional corrections via the option method.


Python
import doubleml as dml

import numpy as np

from sklearn.base import clone

from sklearn.linear_model import LassoCV

# Simulate data
np.random.seed(1234)

n_obs = 500

n_vars = 100

X = np.random.normal(size=(n_obs, n_vars))

theta = np.array([3., 3., 3.])

y = np.dot(X[:, :3], theta) + np.random.standard_normal(size=(n_obs,))

dml_data = dml.DoubleMLData.from_arrays(X[:, 10:], y, X[:, :10])

learner = LassoCV()

ml_l = clone(learner)

ml_m = clone(learner)

dml_plr = dml.DoubleMLPLR(dml_data, ml_l, ml_m)

print(dml_plr.fit().bootstrap().confint(joint=True))
        2.5 %    97.5 %
d1   2.813342  3.055680
d2   2.815224  3.083258
d3   2.860663  3.109069
d4  -0.141546  0.091391
d5  -0.060845  0.176929
d6  -0.158697  0.078474
d7  -0.172022  0.062964
d8  -0.067721  0.174499
d9  -0.092365  0.139491
d10 -0.110717  0.138698

print(dml_plr.p_adjust())
       thetas   pval
d1   2.934511  0.000
d2   2.949241  0.000
d3   2.984866  0.000
d4  -0.025077  0.902
d5   0.058042  0.784
d6  -0.040112  0.808
d7  -0.054529  0.784
d8   0.053389  0.784
d9   0.023563  0.902
d10  0.013990  0.902

print(dml_plr.p_adjust(method='bonferroni'))
       thetas  pval
d1   2.934511   0.0
d2   2.949241   0.0
d3   2.984866   0.0
d4  -0.025077   1.0
d5   0.058042   1.0
d6  -0.040112   1.0
d7  -0.054529   1.0
d8   0.053389   1.0
d9   0.023563   1.0
d10  0.013990   1.0

R
8.3. Simultaneous inference over different DoubleML models (advanced)
The DoubleML package provides a method to perform valid simultaneous inference over different DoubleML models.

Note

Remark that the confidence intervals will generally only be valid if the stronger (uniform) assumptions on e.g. nuisance estimates are satisfied. Further, the models should be estimated on the same data set.

The doubleml.DoubleML class contains a framework attribute which stores a doubleml.DoubleMLFramework object. This object contains a scaled version of the score function

which is used to construct confidence intervals. The framework objects can be concatenated using the doubleml.concat() function.


Python
import doubleml as dml

import numpy as np

from sklearn.base import clone

from sklearn.linear_model import LassoCV

from sklearn.ensemble import RandomForestRegressor

import doubleml as dml

# Simulate data
np.random.seed(1234)

n_obs = 500

n_vars = 100

X = np.random.normal(size=(n_obs, n_vars))

theta = np.array([3., 3., 3.])

y = np.dot(X[:, :3], theta) + np.random.standard_normal(size=(n_obs,))

dml_data = dml.DoubleMLData.from_arrays(X[:, 10:], y, X[:, :10])

learner = LassoCV()

dml_plr_1 = dml.DoubleMLPLR(dml_data, clone(learner), clone(learner))

learner_rf = RandomForestRegressor()

dml_plr_2 = dml.DoubleMLPLR(dml_data, clone(learner_rf), clone(learner_rf))

dml_plr_1.fit()
Out[54]: <doubleml.plm.plr.DoubleMLPLR at 0x7fcce8ab3fb0>

dml_plr_2.fit()
Out[55]: <doubleml.plm.plr.DoubleMLPLR at 0x7fcce8ab2690>

dml_combined = dml.concat([dml_plr_1.framework, dml_plr_2.framework])

dml_combined.bootstrap().confint(joint=True)
Out[57]: 
       2.5 %    97.5 %
0   2.797737  3.071285
1   2.797965  3.100517
2   2.844667  3.125065
3  -0.156545  0.106391
4  -0.076156  0.192240
5  -0.173969  0.093746
6  -0.187153  0.078096
7  -0.083318  0.190096
8  -0.107295  0.154421
9  -0.126777  0.154758
10  2.634577  3.054348
11  2.676534  3.178704
12  2.771157  3.215967
13 -0.360065  0.157091
14 -0.167993  0.358799
15 -0.402113  0.199458
16 -0.313056  0.238225
17 -0.266922  0.318584
18 -0.181446  0.385240
19 -0.280514  0.321686
Frameworks can also be added or subtracted from each other. Of course, this changes the estimated parameter and should be used with caution.


Python
import doubleml as dml

import numpy as np

from sklearn.base import clone

from sklearn.linear_model import LassoCV

from sklearn.ensemble import RandomForestRegressor

import doubleml as dml

# Simulate data
np.random.seed(1234)

n_obs = 500

n_vars = 100

X = np.random.normal(size=(n_obs, n_vars))

theta = np.array([3., 3., 3.])

y = np.dot(X[:, :3], theta) + np.random.standard_normal(size=(n_obs,))

dml_data = dml.DoubleMLData.from_arrays(X[:, 10:], y, X[:, :10])

learner = LassoCV()

dml_plr_1 = dml.DoubleMLPLR(dml_data, clone(learner), clone(learner))

learner_rf = RandomForestRegressor()

dml_plr_2 = dml.DoubleMLPLR(dml_data, clone(learner_rf), clone(learner_rf))

dml_plr_1.fit()
Out[75]: <doubleml.plm.plr.DoubleMLPLR at 0x7fccdba0ba10>

dml_plr_2.fit()
Out[76]: <doubleml.plm.plr.DoubleMLPLR at 0x7fcce8ebb0b0>

dml_combined = dml_plr_1.framework - dml_plr_2.framework

dml_combined.bootstrap().confint(joint=True)
Out[78]: 
      2.5 %    97.5 %
0 -0.074304  0.254400
1 -0.145748  0.188991
2 -0.164034  0.146641
3 -0.122777  0.275596
4 -0.242815  0.168092
5 -0.178934  0.301366
6 -0.242139  0.207912
7 -0.197484  0.252601
8 -0.292047  0.135379
9 -0.244622  0.231430
One possible use case is to substract the estimates from two average potential outcome models as e.g. in the DoubleMLQTE example.

This also works for multiple repetitions if both models have the same number of repetitions, as each repetition is treated seperately.

8.4. References
Belloni, A., Chernozhukov, V., Chetverikov, D., Wei, Y. (2018), Uniformly valid post-regularization confidence regions for many functional parameters in z-estimation framework. The Annals of Statistics, 46 (6B): 3643-75, doi: 10.1214/17-AOS1671.

Chernozhukov, V., Chetverikov, D., Kato, K. (2013). Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics 41 (6): 2786-2819, doi: 10.1214/13-AOS1161.

Chernozhukov, V., Chetverikov, D., Kato, K. (2014), Gaussian approximation of suprema of empirical processes. The Annals of Statistics 42 (4): 1564-97, doi: 10.1214/14-AOS1230.

previous

7. Learners, hyperparameters and hyperparameter tuning

next

9. Sample-splitting, cross-fitting and repeated cross-fitting

 On this page
8.1. Variance estimation
8.2. Confidence bands and multiplier bootstrap for valid simultaneous inference
8.3. Simultaneous inference over different DoubleML models (advanced)
8.4. References
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
9. Sample-splitting, cross-fitting and repeated cross-fitting
Sample-splitting and the application of cross-fitting is a central part of double/debiased machine learning (DML). For all DML models DoubleMLPLR, DoubleMLPLIV, DoubleMLIRM, and DoubleMLIIVM, the specification is done via the parameters n_folds and n_rep. Advanced resampling techniques can be obtained via the boolean parameters draw_sample_splitting and apply_cross_fitting as well as the methods draw_sample_splitting() and set_sample_splitting().

As an example we consider a partially linear regression model (PLR) implemented in DoubleMLPLR.


Python
import doubleml as dml

import numpy as np

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

np.random.seed(1234)

obj_dml_data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=100)

R
9.1. Cross-fitting with K folds
The default setting is n_folds = 5 and n_rep = 1, i.e., 
 folds and no repeated cross-fitting.


Python
dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m, n_folds = 5, n_rep = 1)

print(dml_plr_obj.n_folds)
5

print(dml_plr_obj.n_rep)
1

R
During the initialization of a DML model like DoubleMLPLR a 
-fold random partition 
 of observation indices is generated. The 
-fold random partition is stored in the smpls attribute of the DML model object.


Python
print(dml_plr_obj.smpls)
[[(array([ 0,  2,  3,  4,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19,
       21, 22, 23, 24, 25, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 40, 44,
       46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63, 64,
       65, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85,
       86, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99]), array([ 1,  5, 18, 20, 26, 30, 33, 39, 41, 42, 43, 45, 56, 57, 66, 73, 78,
       83, 87, 96])), (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,
       18, 19, 20, 21, 22, 26, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40,
       41, 42, 43, 44, 45, 46, 48, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63,
       65, 66, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87,
       88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]), array([ 9, 23, 24, 25, 27, 28, 35, 47, 49, 50, 51, 52, 59, 64, 67, 68, 74,
       75, 76, 86])), (array([ 0,  1,  2,  3,  5,  6,  7,  9, 10, 11, 12, 14, 16, 17, 18, 20, 21,
       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 35, 38, 39, 40, 41, 42,
       43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 62,
       63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81,
       83, 84, 86, 87, 88, 89, 90, 91, 92, 96, 98, 99]), array([ 4,  8, 13, 15, 19, 32, 34, 36, 37, 46, 54, 55, 70, 72, 82, 85, 93,
       94, 95, 97])), (array([ 0,  1,  3,  4,  5,  6,  8,  9, 13, 14, 15, 17, 18, 19, 20, 21, 22,
       23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41,
       42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 64,
       65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82,
       83, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98]), array([ 2,  7, 10, 11, 12, 16, 38, 40, 48, 58, 60, 61, 62, 63, 81, 84, 90,
       91, 92, 99])), (array([ 1,  2,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 23,
       24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
       43, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61,
       62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 75, 76, 78, 81, 82, 83, 84,
       85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 99]), array([ 0,  3,  6, 14, 17, 21, 22, 29, 31, 44, 53, 65, 69, 71, 77, 79, 80,
       88, 89, 98]))]]

R
For each 
 the nuisance ML estimator

is based on the observations of all other 
 folds. The values of the two score function components 
 and 
 for each observation index 
 are computed and stored in the attributes psi_a and psi_b.


Python
dml_plr_obj.fit();

print(dml_plr_obj.psi_elements['psi_a'][:5])
[[[-1.78711285e+00]]

 [[-1.41918406e+00]]

 [[-1.95062986e-02]]

 [[-5.41989983e-04]]

 [[-5.91438767e+00]]]

print(dml_plr_obj.psi_elements['psi_b'][:5])
[[[-0.38818693]]

 [[-1.14000073]]

 [[ 0.06692492]]

 [[ 0.00778625]]

 [[ 3.58241568]]]

R
9.2. Repeated cross-fitting with K folds and M repetitions
Repeated cross-fitting is obtained by choosing a value 
 for the number of repetition n_rep. It results in 
 random 
-fold partitions being drawn.


Python
dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m, n_folds = 5, n_rep = 10)

print(dml_plr_obj.n_folds)
5

print(dml_plr_obj.n_rep)
10

R
For each of the 
 partitions, the nuisance ML models are estimated and score functions computed as described in Cross-fitting with K folds. The resulting values of the score functions are stored in 3-dimensional arrays psi_a and psi_b, where the row index corresponds the observation index 
 and the column index to the partition 
. The third dimension refers to the treatment variable and becomes non-singleton in case of multiple treatment variables.


Python
dml_plr_obj.fit();

print(dml_plr_obj.psi_elements['psi_a'][:5, :, 0])
[[-1.80696592e+00 -1.41525168e+00 -2.19509680e+00 -1.95401167e+00
  -9.11199615e-01 -1.46722576e+00 -1.77289874e+00 -1.40127723e+00
  -1.69840389e+00 -1.56387280e+00]
 [-2.76702611e+00 -2.56390147e+00 -1.64476745e+00 -1.52343523e+00
  -2.50398782e+00 -1.49596416e+00 -1.21669513e+00 -2.19374710e+00
  -1.82666866e+00 -2.50093148e+00]
 [-4.38973512e-02 -1.01916030e-01 -4.23690345e-03 -1.51243406e-03
  -3.19073905e-02 -1.94309994e-04 -4.68562150e-02 -8.22507006e-02
  -2.03920960e-02 -5.99549118e-02]
 [-4.92881435e-03 -3.43231359e-03 -7.36566025e-03 -3.21257396e-02
  -2.72875815e-02 -3.36557195e-04 -1.49270769e-02 -9.33335939e-02
  -9.09879814e-04 -1.77401500e-05]
 [-4.07564554e+00 -4.12223182e+00 -5.79792890e+00 -4.88125046e+00
  -5.10637173e+00 -6.12980769e+00 -4.77227783e+00 -5.63245862e+00
  -3.84930915e+00 -5.41798768e+00]]

print(dml_plr_obj.psi_elements['psi_b'][:5, :, 0])
[[-2.88988263e-01 -2.65557405e-01 -7.79458848e-02 -4.18888149e-01
  -4.13137893e-01 -3.94441007e-01 -3.39010121e-02 -1.74461783e-01
  -2.85911521e-01 -3.36231307e-01]
 [-9.79338596e-01 -1.44563945e+00 -1.35620768e+00 -1.44713577e+00
  -1.16539906e+00 -1.07544271e+00 -8.08602774e-01 -1.47966100e+00
  -1.08181827e+00 -9.22272803e-01]
 [ 3.30031116e-02  5.04682310e-02 -1.69562150e-02  1.27951256e-02
  -5.69140475e-02  1.72155839e-03  9.32950022e-02  1.12196389e-02
   8.90963122e-02  3.13034980e-02]
 [-1.80714504e-02 -1.98505871e-02  4.07727773e-02  7.39621961e-02
   9.95372559e-02 -4.59307502e-03 -4.86424193e-02  1.57592948e-01
  -1.68554404e-02  1.99571372e-03]
 [ 1.54517706e+00  1.76444177e+00  1.87674597e+00  1.76419024e+00
   2.02079162e+00  2.87290240e+00  1.59199423e+00  2.23751359e+00
   7.73764317e-01  3.18678094e+00]]

R
We estimate the causal parameter 
 for each of the 
 partitions with a DML algorithm as described in Double machine learning algorithms. Standard errors are obtained as described in Variance estimation and confidence intervals. The aggregation of the estimates of the causal parameter and its standard errors is done using the median

The estimate of the causal parameter 
 is stored in the coef attribute and the asymptotic standard error 
 in se.


Python
In python, the confidence intervals and p-values are based on the doubleml.DoubleMLFramework object. This class provides methods such as confint, bootstrap or p_adjust. For different repetitions, the computations are done separately and combined via the median (based on Chernozhukov et al., 2018).

The estimate of the asymptotic standard error 
 is then based on the median aggregated confidence intervals with crictial value 
, i.e.,

Remark that methods such as methods such as confint, bootstrap or p_adjust do not use the estimate of the standard error.

print(dml_plr_obj.coef)
[0.45957837]

print(dml_plr_obj.se)
[0.07781396]

R
The parameter estimates 
 and asymptotic standard errors 
 for each of the 
 partitions are stored in the attributes _all_coef and _all_se, respectively.


Python
print(dml_plr_obj._all_coef)
[[0.45467447 0.4065173  0.46709481 0.47857478 0.41093655 0.47759584
  0.46618738 0.38990574 0.44890536 0.46448227]]

print(dml_plr_obj._all_se)
[[0.07978296 0.07919896 0.08031571 0.08664208 0.08154161 0.07691847
  0.07436521 0.08091581 0.08333617 0.07685043]]

R
9.3. Externally provide a sample splitting / partition
All DML models allow a partition to be provided externally via the method set_sample_splitting(). In Python we can for example use the K-Folds cross-validator of sklearn KFold in order to generate a sample splitting and provide it to the DML model object. Note that by setting draw_sample_splitting = False one can prevent that a partition is drawn during initialization of the DML model object. The following calls are equivalent. In the first sample code, we use the standard interface and draw the sample-splitting with 
 folds during initialization of the DoubleMLPLR object.


Python
np.random.seed(314)

dml_plr_obj_internal = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m, n_folds = 4)

print(dml_plr_obj_internal.fit().summary)
       coef   std err         t         P>|t|     2.5 %    97.5 %
d  0.424127  0.082297  5.153639  2.554793e-07  0.262829  0.585426

R
In the second sample code, we use the K-Folds cross-validator of sklearn KFold and set the partition via the set_sample_splitting() method.


Python
dml_plr_obj_external = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m, draw_sample_splitting = False)

from sklearn.model_selection import KFold

np.random.seed(314)

kf = KFold(n_splits=4, shuffle=True)

smpls = [(train, test) for train, test in kf.split(obj_dml_data.x)]

dml_plr_obj_external.set_sample_splitting(smpls);

print(dml_plr_obj_external.fit().summary)
       coef   std err         t         P>|t|     2.5 %    97.5 %
d  0.424127  0.082297  5.153639  2.554793e-07  0.262829  0.585426

R
9.4. Sample-splitting without cross-fitting
The boolean flag apply_cross_fitting allows to estimate DML models without applying cross-fitting. It results in randomly splitting the sample into two parts. The first half of the data is used for the estimation of the nuisance ML models and the second half for estimating the causal parameter. Note that cross-fitting performs well empirically and is recommended to remove bias induced by overfitting, see also Sample splitting to remove bias induced by overfitting.


Python
Note

The flag apply_cross_fitting is deprecated for the python package. To avoid cross-fitting, please use the option to set external predictions.


R
9.5. Estimate DML models without sample-splitting
The implementation of the DML models allows the estimation without sample splitting, i.e., all observations are used for learning the nuisance models as well as for the estimation of the causal parameter. Note that this approach usually results in a bias and is therefore not recommended without appropriate theoretical justification, see also Sample splitting to remove bias induced by overfitting.


Python
Note

The flag apply_cross_fitting is deprecated for the python package. To avoid cross-fitting, please use the option to set external predictions. Additionally, the number of folds n_folds is expected to be at least 2.


R
9.6. References
Chernozhukov, Victor and Demirer, Mert and Duflo, Esther and Fernández-Val, Iván (2018), Generic Machine Learning Inference on Heterogeneous Treatment Effects in Randomized Experiments, with an Application to Immunization in India, National Bureau of Economic Research, doi: 10.3386/w24678.

previous

8. Variance estimation and confidence intervals

next

10. Sensitivity analysis

 On this page
9.1. Cross-fitting with K folds
9.2. Repeated cross-fitting with K folds and M repetitions
9.3. Externally provide a sample splitting / partition
9.4. Sample-splitting without cross-fitting
9.5. Estimate DML models without sample-splitting
9.6. References
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.

Skip to main content
Interested to learn more? We offer DoubleML Trainings!

DoubleML

Install
Getting Started
Workflow
User Guide
Examples
Python API
Search the docs ...
⌘
+
K
GitHub
PyPI
Discussions
Logo

Search the docs ...
⌘
+
K
Section Navigation

1. The basics of double/debiased machine learning
2. Data Backend
3. Models
4. Heterogeneous Treatment Effects
5. Score functions
6. Double machine learning algorithms
7. Learners, hyperparameters and hyperparameter tuning
8. Variance estimation and confidence intervals
9. Sample-splitting, cross-fitting and repeated cross-fitting
10. Sensitivity Analysis
User Guide
10. Sensitivity analysis
The DoubleML package implements sensitivity analysis with respect to omitted variable bias based on Chernozhukov et al. (2022).

10.1. General algorithm
The section Theory contains a general summary and the relevant defintions, whereas Implementation considers the general part of the implementation.

10.1.1. Theory
Assume that we can write the model in the following representation

where usually 
 (currently, the sensitivity analysis is only available for linear models). As long as 
 is a continuous linear functional of 
, there exists a unique square integrable random variable 
, called Riesz representer (see Riesz-Fréchet representation theorem), such that

The target parameter 
 has the following representation

which corresponds to a Neyman orthogonal score function (orthogonal with respect to nuisance elements 
). To bound the omitted variable bias, the following further elements are needed. The variance of the outcome regression

and the second moment of the Riesz representer

Both representations are Neyman orthogonal with respect to 
 and 
, respectively. Further, define the corresponding score functions

 
Recall that the parameter 
 is identified via the moment condition

If 
 does not include all confounding variables, the “true” target parameter 
 would only be identified via the extendend (or “long”) form

where 
 includes the unobserved counfounders 
. In Theorem 2 of their paper Chernozhukov et al. (2022) are able to bound the omitted variable bias

where

denotes the product of additional variations in the outcome regression and Riesz representer generated by omitted confounders and

denotes the correlations between the deviations generated by omitted confounders. The choice 
 is conservative and accounts for adversarial confounding. Further, the bound can be expressed as

where

 
 
 
 
 
 
 
As 
 and 
 do not depend on the unobserved confounders 
 they are identified. Further, the other parts have the following interpretations

cf_y
 
 measures the proportion of residual variance in the outcome 
 explained by the latent confounders 

cf_d
 
 measures the proportion of residual variance in the Riesz representer 
 generated by the latent confounders 

Note

cf_y has the interpretation as the nonparametric partial 
 of 
 with 
 given 

 
For model-specific interpretations of cf_d or 
, see the corresponding chapters (e.g. Partially linear regression model (PLR)).

Consequently, for given values cf_y and cf_d, we can create lower and upper bounds for target parameter 
 of the form

Let 
 the (correctly scaled) score function for the target parameter 
. Then

 
determines a orthongonal score function for 
, with nuisance elements 
. The score can be used to calculate the standard deviations of 
 via

For more detail and interpretations see Chernozhukov et al. (2022).

10.1.2. Implementation
The Partially linear regression model (PLR) will be used as an example


Python
import numpy as np

import doubleml as dml

from doubleml.datasets import make_plr_CCDDHNR2018

from sklearn.ensemble import RandomForestRegressor

from sklearn.base import clone

learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)

ml_l = clone(learner)

ml_m = clone(learner)

np.random.seed(1111)

data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20, return_type='DataFrame')

obj_dml_data = dml.DoubleMLData(data, 'y', 'd')

dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_l, ml_m)
If the sensitivity analysis is implemented (see Model-specific implementations), the corresponding sensitivity elements are estimated automatically by calling the fit() method. In most cases these elements are based on the following plug-in estimators

 
 
 
where 
 and 
 denote the cross-fitted predictions of the outcome regression and the Riesz representer (both are model specific, see Model-specific implementations). Further, the corresponding scores are defined as

 
After the fit() call, the sensitivity elements are stored in a dictionary and can be accessed via the sensitivity_elements property.


Python
dml_plr_obj.fit()
Out[13]: <doubleml.plm.plr.DoubleMLPLR at 0x7fcce8ab3dd0>

dml_plr_obj.sensitivity_elements.keys()
Out[14]: dict_keys(['sigma2', 'nu2', 'psi_sigma2', 'psi_nu2', 'riesz_rep'])
Each value is a 
-dimensional array, with the variances being of form (1, n_rep, n_coefs) and the scores of form (n_obs, n_rep, n_coefs). The sensitivity_analysis() method then computes the upper and lower bounds for the estimate, based on the sensitivity parameters cf_y, cf_d and rho (default is rho=1.0 to account for adversarial confounding). Additionally, one-sided confidence bounds are computed based on a supplied significance level (default level=0.95). The results are summarized as a formatted string in the sensitivity_summary


Python
dml_plr_obj.sensitivity_analysis(cf_y=0.03, cf_d=0.03, rho=1.0, level=0.95)
Out[15]: <doubleml.plm.plr.DoubleMLPLR at 0x7fcce8ab3dd0>

print(dml_plr_obj.sensitivity_summary)
================== Sensitivity Analysis ==================

------------------ Scenario          ------------------
Significance Level: level=0.95
Sensitivity parameters: cf_y=0.03; cf_d=0.03, rho=1.0

------------------ Bounds with CI    ------------------
   CI lower  theta lower     theta  theta upper  CI upper
d  0.408476     0.482461  0.512672     0.542883  0.616698

------------------ Robustness Values ------------------
   H_0     RV (%)    RVa (%)
d  0.0  40.029364  35.077502
or can be directly accessed via the sensitivity_params property.


Python
dml_plr_obj.sensitivity_params
Out[17]: 
{'theta': {'lower': array([0.48246134]), 'upper': array([0.5428834])},
 'se': {'lower': array([0.04497975]), 'upper': array([0.04487585])},
 'ci': {'lower': array([0.40847623]), 'upper': array([0.61669761])},
 'rv': array([0.40029364]),
 'rva': array([0.35077502]),
 'input': {'cf_y': 0.03,
  'cf_d': 0.03,
  'rho': 1.0,
  'level': 0.95,
  'null_hypothesis': array([0.])}}
The bounds are saved as a nested dictionary, where the keys 'theta' denote the bounds on the parameter 
, 'se' denotes the corresponding standard error and 'ci' denotes the lower and upper confidence bounds for 
. Each of the keys refers to a dictionary with keys 'lower' and 'upper' which refer to the lower or upper bound, e.g. sensitivity_params['theta']['lower'] refers to the lower bound 
 of the estimated cofficient .

Further, the sensitivity analysis has an input parameter theta (with default theta=0.0), which refers to the null hypothesis used for each coefficient. This null hypothesis is used to calculate the robustness values as displayed in the sensitivity_params.

The robustness value 
 is defined as the required confounding strength (cf_y=rv and cf_d=rv), such that the lower or upper bound of the causal parameter includes the null hypothesis. If the estimated parameter 
 is larger than the null hypothesis the lower bound is used and vice versa. The robustness value 
 defined analogous, but additionally incorporates statistical uncertainty (as it is based on the confidence intervals of the bounds).

To obtain a more complete overview over the sensitivity one can call the sensitivity_plot() method. The methods creates a contour plot, which calculates estimate of the upper or lower bound for 
 (based on the null hypothesis) for each combination of cf_y and cf_d in a grid of values.

Contour plot
Contour plot example (see Examples)
By adjusting the parameter value='ci' in the sensitivity_plot() method the bounds are displayed for the corresponding confidence level.

Note

The sensitivity_plot() requires to call sensitivity_analysis first, since the choice of the bound (upper or lower) is based on the corresponding null hypothesis. Further, the parameters rho and level are used. Both are contained in the sensitivity_params property.

The sensitivity_plot() is created for the first treatment variable. This can be changed via the idx_treatment parameter.

The robustness values are given via the intersection countour of the null hypothesis and the identity.

10.1.3. Benchmarking
The input parameters for the sensitivity analysis are quite hard to interpret (depending on the model). Consequently it is challenging to come up with reasonable bounds for the confounding strength cf_y and cf_d (and rho). To get a grasp on the magnitude of the bounds a popular approach is to rely on observed confounders to obtain an informed guess on the strength of possibly unobserved confounders.

The underlying principle is relatively simple. If we have an observed confounder 
, we are able to emulate omitted confounding by purposely omitting 
 and refitting the whole model. This enables us to compare the “long” and “short” form with and without omitted confounding. Considering the sensitivity_params of both models one can estimate the corresponding strength of confounding cf_y and cf_d (and rho).

Note

The benchmarking can also be done with a set of benchmarking variables (e.g. 
), which tries to emulate the effect of multiple unobserved confounders.

The approach is quite computationally demanding, as the short model that omits the benchmark variables has to be fitted.

The sensitivity_benchmark() method implements this approach. The method just requires a set of valid covariates, the benchmarking_set, to compute the benchmark. The benchmark variables have to be a subset of the covariates used in the main analysis.


Python
dml_plr_obj.sensitivity_benchmark(benchmarking_set=["X1"])
Out[1]: 
       cf_y      cf_d  rho  delta_theta
d  0.024364  0.455981  1.0     0.091406
The method returns a pandas.DataFrame, containing the benchmarked values for cf_y, cf_d, rho and the change in the estimates delta_theta.

Note

The benchmarking results should be used to get an idea of the magnitude/validity of proposed confounding strength of the omitted confounders. Whether these values are close to the real confounding, depends entirely on the setting and choice of the benchmarking variables. A good benchmarking set has a strong justification which refers to the omitted confounders.

If the benchmarking variables are only weak confounders, the estimates of rho can be slightly unstable (due to small denominators).

The implementation is based on Chernozhukov et al. (2022) Appendix D and corresponds to a generalization of the benchmarking process in the Sensemakr package for regression models to the use with double machine learning. For an introduction to Sensemakr see Cinelli and Hazlett (2020) and the Sensemakr introduction.

The benchmarked estimates are the following:

Let the subscript 
, denote the “short” form of the model, where the benchmarking variables are omitted.

 denotes the variance of the outcome regression in the “short” form.

 denotes the second moment of the Riesz representer in the “short” form.

Both parameters are contained in the sensitivity_params of the “short” form. This enables the following estimation of the nonparametric 
’s of the outcome regression

 

 

and the correlation ratio of the estimated Riesz representations

 
The benchmarked estimates are then defined as

cf_y
 
 measures the proportion of residual variance in the outcome 
 explained by adding the purposely omitted benchmarking_set

cf_d
 
 measures the proportional gain in variation that the benchmarking_set creates in the Riesz representer

Further, the degree of adversity 
 can be estimated via

 
For a more detailed description, see Chernozhukov et al. (2022) Appendix D.

Note

As benchmarking requires the estimation of a seperate model, the use with external predictions is generally not possible, without supplying further predictions.

10.2. Model-specific implementations
This section contains the implementation details for each specific model and model specific interpretations.

10.2.1. Partially linear models (PLM)
The following partially linear models are implemented.

10.2.1.1. Partially linear regression model (PLR)
In the Partially linear regression model (PLR) the confounding strength cf_d can be further be simplified to match the explanation of cf_y. Given the that the Riesz representer takes the following form

 
 
 
 
 
one can show that

 
 
 
Therefore,

cf_y
 
 measures the proportion of residual variance in the outcome 
 explained by the latent confounders 

cf_d
 
 measures the proportion of residual variance in the treatment 
 explained by the latent confounders 

Note

In the Partially linear regression model (PLR), both cf_y and cf_d can be interpreted as nonparametric partial 

cf_y has the interpretation as the nonparametric partial 
 of 
 with 
 given 

 
cf_d has the interpretation as the nonparametric partial 
 of 
 with 
 given 

 
Using the partially linear regression model with score='partialling out' the nuisance_elements are implemented in the following form

 
 
 
 
with scores

 
 
 
If score='IV-type' the senstivity elements are instead set to

 
 
 
10.2.2. Interactive regression models (IRM)
The following nonparametric regression models implemented.

10.2.2.1. Interactive regression model (IRM)
In the Binary Interactive Regression Model (IRM) the target parameter can be written as

where 
 are weights (e.g. set to 
 for the ATE). This implies the following representations

 
 
 
 
 
Note

In the Binary Interactive Regression Model (IRM) with for the ATE (weights equal to 
), the form and interpretation of cf_y is the same as in the Partially linear regression model (PLR).

cf_y has the interpretation as the nonparametric partial 
 of 
 with 
 given 

 
cf_d takes the following form

 
where the numerator measures the gain in average conditional precision to predict 
 by using 
 in addition to 
. The denominator is the average conditional precision to predict 
 by using 
 and 
. Consequently cf_d measures the relative gain in average conditional precision.

Remark that 
 denotes the variance of the conditional distribution of 
 given 
, such that the inverse measures the precision of predicting 
 conditional on 
.

Since 
 
, this corresponds to

 
which has the same numerator but is instead relative to the average conditional precision to predict 
 by using only 
.

Including weights changes only the definition of cf_d to

 
 
 
 
which has a interpretation as the relative weighted gain in average conditional precision.

The nuisance_elements are then computed with plug-in versions according to the general Implementation. For score='ATE', the weights are set to one

wheras for score='ATTE'

 
such that

 
10.2.2.2. Average Potential Outcomes (APOs)
In the Binary Interactive Regression Model (IRM) the (weighted) average potential outcome for the treatment level 
 can be written as

where 
 are weights (e.g. set to 
 for the APO). This implies the following representations

 
 
 
 
Note

In the Binary Interactive Regression Model (IRM) the form and interpretation of cf_y only depends on the conditional expectation 
.

cf_y has the interpretation as the nonparametric partial 
 of 
 with 
 given 

 
cf_d takes the following form

 
 
 
 
where the numerator measures the average change in inverse propensity weights for 
 conditional on 
 in addition to 
. The denominator is the average inverse propensity weights for 
 conditional on 
 and 
. Consequently cf_d measures the relative change in inverse propensity weights. Including weights changes only the definition of cf_d to

 
 
 
 
which has a interpretation as the relative weighted change in inverse propensity weights.

The nuisance_elements are then computed with plug-in versions according to the general Implementation. The default weights are set to one

whereas

have to be supplied for weights which depend on 
 or 
.

10.2.3. Difference-in-Differences Models
The following difference-in-differences models implemented.

Note

Remark that Benchmarking is only relevant for score='observational', since no effect of 
 on treatment assignment is assumed. Generally, we recommend score='observational', if unobserved confounding seems plausible.

10.2.3.1. Difference-in-Differences for Panel Data
For a detailed description of the scores and nuisance elements, see Panel Data.

In the Panel data with score='observational' and in_sample_normalization=True the score function implies the following representations

 
 
 
 
 
 
 
 
If instead in_sample_normalization=False, the Riesz representer changes to

 
 
For score='experimental' implies the score function implies the following representations

 
 
 
 
 
The nuisance_elements are then computed with plug-in versions according to the general Implementation.

Note

Remark that the elements are only non-zero for units in the corresponding treatment group 
 and control group 
, as 
 if 
.

10.2.3.2. Difference-in-Differences for repeated cross-sections
Note

Will be implemented soon.

10.2.3.3. Two treatment periods
Warning

This documentation refers to the deprecated implementation for two time periods. This functionality will be removed in a future version. The generalized version are Difference-in-Differences for Panel Data and Difference-in-Differences for repeated cross-sections.

10.2.3.3.1. Panel Data
In the Panel data with score='observational' and in_sample_normalization=True the score function implies the following representations

 
 
 
 
 
 
 
 
If instead in_sample_normalization=False, the Riesz representer changes to

 
 
For score='experimental' implies the score function implies the following representations

 
 
 
 
 
The nuisance_elements are then computed with plug-in versions according to the general Implementation.

10.2.3.3.2. Repeated Cross-Sectional Data
In the Repeated cross-sections with score='observational' and in_sample_normalization=True the score function implies the following representations

 
 
 
 
 
 
 
 
 
 
If instead in_sample_normalization=False, the Riesz representer (after simplifications) changes to

 
 
 
For score='experimental' and in_sample_normalization=True implies the score function implies the following representations

 
 
 
 
 
 
 
And again, if instead in_sample_normalization=False, the Riesz representer (after simplifications) changes to

 
 
 
 
The nuisance_elements are then computed with plug-in versions according to the general Implementation.

previous

9. Sample-splitting, cross-fitting and repeated cross-fitting

next

Examples

 On this page
10.1. General algorithm
10.1.1. Theory
10.1.2. Implementation
10.1.3. Benchmarking
10.2. Model-specific implementations
10.2.1. Partially linear models (PLM)
10.2.2. Interactive regression models (IRM)
10.2.3. Difference-in-Differences Models
 Edit on GitHub
 Show Source
© Copyright 2023, Bach, P., Chernozhukov, V., Klaassen, S., Kurz, M. S., and Spindler, M..

Created using Sphinx 8.1.3.

Built with the PyData Sphinx Theme 0.15.4.